{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained and concatenated analysis F3C F4B\n",
    "\n",
    "This analysis is very time consuming and automatic caching was used to make things run smoothly. Some of the code is structured in a slightly weird way to re-use and modify existing cached results, to save time.\n",
    "\n",
    "Without caching ahead of time, this notebook should take about a day to run on a typical desktop machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data location is /home/mer49/Workspace2/PPC_data/\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Add local scripts to path\n",
    "import os,sys\n",
    "sys.path.insert(0,os.path.abspath(\"./\"))\n",
    "import neurotools\n",
    "\n",
    "# Set up cache\n",
    "from neurotools.jobs.initialize_system_cache import initialize_caches,cache_test\n",
    "PYCACHEDIR = os.path.abspath('./')\n",
    "CACHENAME  = 'PPC_cache'\n",
    "from neurotools.tools import ensure_dir\n",
    "ensure_dir(PYCACHEDIR+os.sep+CACHENAME)\n",
    "initialize_caches(\n",
    "    level1  = PYCACHEDIR,\n",
    "    force   = False,\n",
    "    verbose = False,\n",
    "    CACHE_IDENTIFIER = CACHENAME)\n",
    "\n",
    "# Import libraries\n",
    "from neurotools.nlab import *\n",
    "import ppc_data_loader\n",
    "\n",
    "# Set this to the location of the PPC data on your machine\n",
    "ppc_data_loader.path = '/home/mer49/Dropbox (Cambridge University)/Datasets/PPC_data/'\n",
    "from ppc_data_loader   import *\n",
    "from ppc_trial         import *\n",
    "\n",
    "np.seterr(all='raise');\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib configured\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 200\n",
    "TEXTWIDTH = 5.62708\n",
    "matplotlib.rcParams['figure.figsize'] = (TEXTWIDTH, TEXTWIDTH/sqrt(2))\n",
    "import warnings\n",
    "from matplotlib import MatplotlibDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\",category=MatplotlibDeprecationWarning)\n",
    "SMALL_SIZE  = 7.5\n",
    "MEDIUM_SIZE = 8\n",
    "BIGGER_SIZE = 8.5\n",
    "matplotlib.rc('font'  , size     =SMALL_SIZE ) # controls default text sizes\n",
    "matplotlib.rc('axes'  , titlesize=MEDIUM_SIZE) # fontsize of the axes title\n",
    "matplotlib.rc('axes'  , labelsize=MEDIUM_SIZE) # fontsize of the x and y labels\n",
    "matplotlib.rc('xtick' , labelsize=SMALL_SIZE ) # fontsize of the tick labels\n",
    "matplotlib.rc('ytick' , labelsize=SMALL_SIZE ) # fontsize of the tick labels\n",
    "matplotlib.rc('legend', fontsize =SMALL_SIZE ) # legend fontsize\n",
    "matplotlib.rc('figure', titlesize=BIGGER_SIZE) # fontsize of the figure title\n",
    "matplotlib.rc('lines' , solid_capstyle='round')\n",
    "print('Matplotlib configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set spans of (mostly) consecutive days with sufficient No. of units in common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets defined...\n"
     ]
    }
   ],
   "source": [
    "NXVAL   = 10\n",
    "REPL    = 100\n",
    "NGRID   = 20\n",
    "errtype = 'L1'\n",
    "\n",
    "# Use these datasets\n",
    "use = [(1,[1, 4, 5, 6, 7, 10, 14]),\n",
    "       (3,[ 1,  2,  4, 6, 7,  8,  9, 10, 11, 12]),\n",
    "       (4,[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n",
    "       (5,[6, 7, 8, 9, 10, 11, 12])]\n",
    "\n",
    "print('Datasets defined...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constrained analysis subroutines\n",
    "\n",
    "These are parallellized, since the multiple crossvalidations (and permutation samples) are quite labor intensive.\n",
    "\n",
    " - Instead of just talking one set of 10-fold cross validations, we random select 1/10th of the data for validation, and sample this 100 times (to better cover the variability in the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sweep constraint parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined subroutines\n",
      "Defined LMS algorithm\n"
     ]
    }
   ],
   "source": [
    "from ppc_analysis import kininfo,add_constant\n",
    "\n",
    "DEBUG_ON = False\n",
    "def constrained_analysis_sweep(x,y,NGRID=20):\n",
    "    '''\n",
    "    Estimate regularized linear decoders with a penalty\n",
    "    on the change in weights across days. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ww0: \n",
    "        List of initial weight vector guesses based on OLS\n",
    "        on each session separately.\n",
    "    wwc: \n",
    "        Concatenated weight vector guess based on OLS for \n",
    "        all sessions concatenated.\n",
    "    allww: \n",
    "        List of constrained weight fits\n",
    "    ll: \n",
    "        Interpolation weights between the single-day and the\n",
    "        concatenated scenarios. \n",
    "    '''\n",
    "    x = list(map(np.array,x))\n",
    "    y = list(map(np.array,y))\n",
    "    # Same-day fits\n",
    "    ww0 = array([reglstsq(xi,yi) for (xi,yi) in zip(x,y)])[:,:,0]\n",
    "    # Concatenated fit\n",
    "    wc  = reglstsq(cat(x),cat(y))\n",
    "    wwc = outer(ones(len(x)),wc)\n",
    "    # Compute covariances and cross covariances\n",
    "    sYY = np.array([(yi.T @ yi)/yi.shape[0] for  yi     in y       ])\n",
    "    sXX = np.array([(xi.T @ xi)/xi.shape[0] for  xi     in x       ])\n",
    "    sXY = np.array([(xi.T @ yi)/yi.shape[0] for (xi,yi) in zip(x,y)])  \n",
    "    # Baseline values for eror and jacobian of OLS penalty\n",
    "    e0 = np.sum(sYY)\n",
    "    j0 = -2*sXY.ravel()\n",
    "    # Inter-day difference operator,\n",
    "    # defines quadratic penalty on the weigth changes.\n",
    "    D = len(x)\n",
    "    G = (-eye(D)+eye(D,k=1))[:-1,:]\n",
    "    Q = G.T@G\n",
    "    if DEBUG_ON:\n",
    "        print('ww0',ww0.shape)\n",
    "        print('sXX',sXX.shape)\n",
    "        print('Q',Q.shape)\n",
    "    # Error functions for the OLS and Δw penalties\n",
    "    err1 = lambda w:e0+einsum('is,ist,it',w,sXX,w)-2*np.sum(w*sXY)\n",
    "    err2 = lambda w:einsum('si,st,ti',w,Q,w)\n",
    "    # Rescale both OLS and constraint contribution to objective to be similar\n",
    "    # (improved numeric conditioning, able to cover reasonable range of\n",
    "    # constraint values with a fixed grid search)\n",
    "    emin1    = err1(ww0) # Best-case  OLS error: same-day\n",
    "    emax1    = err1(wwc) # Worst-case OLS error: concatenated\n",
    "    emin2    = 0         # Best-case  Δw penalty: concatenated (0 Δw)\n",
    "    emax2    = err2(ww0) # Worst-case Δw penalty: same-day fits\n",
    "    scale_e1 = 1/(emax1-emin1) \n",
    "    scale_e2 = 1/(emax2-emin2) \n",
    "    # Grid search over convex combinations\n",
    "    ll = linspace(0,1,NGRID)\n",
    "    allww = [ww0.ravel(),ww0.ravel()]\n",
    "    for l in ll:\n",
    "        w0 = (2*allww[-1]-allww[-2]).ravel()\n",
    "        # Objective and Jacobian combining the OLS and Δw penalties\n",
    "        a,b = (1-l)*scale_e1, l*scale_e2\n",
    "        def objective(w):\n",
    "            w  = w.reshape(ww0.shape)\n",
    "            return a*(err1(w)-emin1) + b*(err2(w)-emin2)\n",
    "        def jacobian(w):\n",
    "            w  = w.reshape(ww0.shape) \n",
    "            j1 = 2*einsum('ist,it->is',sXX,w).ravel()+j0\n",
    "            j2 = 2*einsum('si,st->ti' ,w,Q  ).ravel()\n",
    "            return a*j1 + b*j2\n",
    "        # (\"minimize_retry\" wraps scipy.optimize.minimize\n",
    "        # It tries faster gradient-based optimizers first, \n",
    "        # but resorts to the simplex algorithm if that fails)\n",
    "        allww.append(\n",
    "            minimize_retry(objective,w0,jacobian,\n",
    "            tol          =1e-6,\n",
    "            show_progress=False,\n",
    "            printerrors  =False))\n",
    "    allww = array(allww[2:]).reshape((NGRID,)+ww0.shape)\n",
    "    return ww0,wwc,allww,ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sweep constraint parameter with crossvalidation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_sweep_crossvalidated(X,Y,NXVAL=10,NGRID=20,errmethod='L1',matched=True):\n",
    "    '''\n",
    "    '''\n",
    "    efn = neurotools.stats.error_functions[errmethod]\n",
    "    X   = [array(x) for x in X]\n",
    "    Y   = [array(y) for y in Y]\n",
    "    D   = len(X)\n",
    "    N   = len(cat(X))\n",
    "    M   = N/D**2/NXVAL\n",
    "    def partition(x,y):\n",
    "        K      = len(x)\n",
    "        trials = arange(K)\n",
    "        Ntest  = int(M           if matched else K/NXVAL)\n",
    "        Ntrain = int(M*(NXVAL-1) if matched else K-Ntest)\n",
    "        train  = int32(np.random.choice(trials,Ntrain,replace=False))\n",
    "        test   = int32(np.random.choice(list(set(trials)-set(train)),Ntest,replace=False))\n",
    "        return x[train],y[train],x[test],y[test]\n",
    "    trnX,trnY,tstX,tstY = [amap(cat,v) for v in zip(*[partition(x,y) for (x,y) in zip(X,Y)])]\n",
    "    ww0,wwc,allww,ll = constrained_analysis_sweep(trnX,trnY,NGRID)\n",
    "    results = cat([[(0,ww0)],list(zip(ll,allww)),[(1,wwc)]])\n",
    "    return [{'MAW'  :mean(abs(w)),\n",
    "             'RMSW' :mean(abs(w)**2)**0.5,\n",
    "             'MADW' :mean(abs(diff(w,axis=0))),\n",
    "             'RMSDW':mean(abs(diff(w,axis=0))**2)**0.5,\n",
    "             'MAY'  :mean([mean(abs(y-mean(y))) for y in tstY]), \n",
    "             'MAE'  :mean([mean(abs(y-x@w)) for (w,x,y) in zip(w,tstX,tstY)]),\n",
    "             'RMSE' :mean([mean((y-x@w)**2) for (w,x,y) in zip(w,tstX,tstY)])**0.5,\n",
    "             'MERR' :mean([efn(y,x@w)       for (w,x,y) in zip(w,tstX,tstY)])} \n",
    "            for (l,w) in results],results\n",
    "\n",
    "@memoize\n",
    "def get_data_constrained_analysis_2(animal,sessions,predict,\n",
    "    permute=False,\n",
    "    split=1,\n",
    "    do_add_constant=True):    \n",
    "    '''\n",
    "    Get data pre-processed for performing the constrained analyses. \n",
    "\n",
    "    We extract good trials, z-score the dF/F calcium signals, and zero-mean\n",
    "    the kinematic variables, within each trial. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    animal: int\n",
    "        Which subject to use\n",
    "    sessions: list of ints\n",
    "        Which sessions to use\n",
    "    predict: int\n",
    "        Which kinematic variable to predict\n",
    "\n",
    "    Other Parametes\n",
    "    ---------------\n",
    "    permute: bool, default False\n",
    "        Whether to randomly scramble the neuronal identities. \n",
    "        Used for shuffle chance level assessment. \n",
    "    split: int, default 1\n",
    "        Split days into `split` pieces. \n",
    "    do_add_constant: bool, default True\n",
    "        Whether to add a constant offset feature to the neural data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X: list\n",
    "        List of neural trial data for each session\n",
    "    Y: list\n",
    "        List of kinematic trial data for each session\n",
    "    '''\n",
    "    # Get units in common\n",
    "    units,uidxs = get_units_in_common(animal,sessions)\n",
    "    X,Y = [],[]\n",
    "    if not permute in {True,False,'pair'}:\n",
    "        raise ValueError('Param `permute` should be True,False, or \"pair\"')\n",
    "    if permute=='pair':\n",
    "        a,b = sorted(choice(range(len(sessions)),2,False))\n",
    "        sessions = [sessions[a],sessions[b]]\n",
    "        permute = True\n",
    "    for s in sessions:\n",
    "        # Get trials for this session\n",
    "        f  = get_dFF(animal,s)[:,units]\n",
    "        if permute:\n",
    "            f = f[:,np.random.permutation(len(units))]\n",
    "        k  = kininfo[predict]['get'](animal,s)\n",
    "        if do_add_constant:\n",
    "            x  = array([add_constant(x) for x in extract_in_trial(f,animal,s,dozscore=True)])\n",
    "        else:\n",
    "            x  = array([x for x in extract_in_trial(f,animal,s,dozscore=True)])\n",
    "        y  = array(extract_in_trial(k,animal,s,dozeromean=True))\n",
    "        n  = len(x)\n",
    "        b  = int(n//split)\n",
    "        for i in range(split):\n",
    "            X += [x[i*b:] if i==split-1 else x[i*b:(i+1)*b]]\n",
    "            Y += [y[i*b:] if i==split-1 else y[i*b:(i+1)*b]]\n",
    "    return X,Y\n",
    "\n",
    "def compute_constrained_sweep(animal,sessions,predict,\n",
    "                              NXVAL=10,\n",
    "                              REPL=1,\n",
    "                              NGRID=20,\n",
    "                              errtype='L1',\n",
    "                              matched=False,\n",
    "                              permute=False,\n",
    "                              split=1):\n",
    "    '''\n",
    "    Helper function for computing constrained models. \n",
    "    '''\n",
    "    # Handle head-direction as a special case: use a circular\n",
    "    # erorr function (otherwise default to L1)\n",
    "    emth = (errtype+'_degrees') if predict==4 else errtype\n",
    "    # Prepare data for analysis\n",
    "    X,Y  = get_data_constrained_analysis_2(animal,sessions,predict,permute,split)\n",
    "    # Fit multiple models, interpolating between single-day and concatenated\n",
    "    return constrained_sweep_crossvalidated(X,Y,NXVAL,NGRID,emth,matched)\n",
    "\n",
    "# Parallel run of all cross-validation samples\n",
    "def constrained_analysis_helper(p):\n",
    "    '''\n",
    "    Small wrapper function to allow parallel computation. \n",
    "    This is necessary for older versions of python\n",
    "    '''\n",
    "    i,p = p\n",
    "    return i,compute_constrained_sweep(*p)[0]\n",
    "\n",
    "@memoize\n",
    "def do_parallel_constrained_analysis(animal,sessions,predict,\n",
    "                                     NXVAL=10,\n",
    "                                     REPL=100,\n",
    "                                     NGRID=20,\n",
    "                                     errtype='L1',\n",
    "                                     matched=False):\n",
    "    '''\n",
    "    '''\n",
    "    animal = animal # weird cache stuff; ignore but don't remove this line\n",
    "    reset_pool()\n",
    "    jobs = [\n",
    "        (animal,sessions,predict,NXVAL,1,NGRID,errtype,matched,False) \n",
    "        for i in range(REPL)\n",
    "        ]+[\n",
    "        (animal,sessions,predict,NXVAL,1,NGRID,errtype,matched,True) \n",
    "        for i in range(REPL)\n",
    "        ]+[\n",
    "        (animal,sessions,predict,NXVAL,1,NGRID,errtype,matched,'pair') \n",
    "        for i in range(REPL)\n",
    "        ]\n",
    "    results = parmap(constrained_analysis_helper,enumerate(jobs),debug=DEBUG_ON)\n",
    "    print('done')\n",
    "    rawresults  = results[REPL*0:REPL*1]\n",
    "    shuffle     = results[REPL*1:REPL*2]\n",
    "    pairshuffle = results[REPL*2:REPL*3]\n",
    "    return rawresults,shuffle,pairshuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export preprocessed data for independent validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M 1 X position\n",
      "Saving to \n",
      "concatenated_analyses_data_M1_sessions_1_4_5_6_7_10_14_days_1_4_5_6_7_11_15_variable_X_position.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 1 Y position\n",
      "Saving to \n",
      "concatenated_analyses_data_M1_sessions_1_4_5_6_7_10_14_days_1_4_5_6_7_11_15_variable_Y_position.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 1 X velocity\n",
      "Saving to \n",
      "concatenated_analyses_data_M1_sessions_1_4_5_6_7_10_14_days_1_4_5_6_7_11_15_variable_X_velocity.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 1 Y velocity\n",
      "Saving to \n",
      "concatenated_analyses_data_M1_sessions_1_4_5_6_7_10_14_days_1_4_5_6_7_11_15_variable_Y_velocity.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 1 Head direction\n",
      "Saving to \n",
      "concatenated_analyses_data_M1_sessions_1_4_5_6_7_10_14_days_1_4_5_6_7_11_15_variable_Head_direction.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 3 X position\n",
      "Saving to \n",
      "concatenated_analyses_data_M3_sessions_1_2_4_6_7_8_9_10_11_12_days_1_3_5_7_8_9_10_12_13_14_variable_X_position.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 3 Y position\n",
      "Saving to \n",
      "concatenated_analyses_data_M3_sessions_1_2_4_6_7_8_9_10_11_12_days_1_3_5_7_8_9_10_12_13_14_variable_Y_position.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 3 X velocity\n",
      "Saving to \n",
      "concatenated_analyses_data_M3_sessions_1_2_4_6_7_8_9_10_11_12_days_1_3_5_7_8_9_10_12_13_14_variable_X_velocity.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 3 Y velocity\n",
      "Saving to \n",
      "concatenated_analyses_data_M3_sessions_1_2_4_6_7_8_9_10_11_12_days_1_3_5_7_8_9_10_12_13_14_variable_Y_velocity.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 3 Head direction\n",
      "Saving to \n",
      "concatenated_analyses_data_M3_sessions_1_2_4_6_7_8_9_10_11_12_days_1_3_5_7_8_9_10_12_13_14_variable_Head_direction.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 4 X position\n",
      "Saving to \n",
      "concatenated_analyses_data_M4_sessions_1_2_3_4_5_6_7_8_9_10_days_1_2_3_4_5_6_7_10_11_12_variable_X_position.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 4 Y position\n",
      "Saving to \n",
      "concatenated_analyses_data_M4_sessions_1_2_3_4_5_6_7_8_9_10_days_1_2_3_4_5_6_7_10_11_12_variable_Y_position.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 4 X velocity\n",
      "Saving to \n",
      "concatenated_analyses_data_M4_sessions_1_2_3_4_5_6_7_8_9_10_days_1_2_3_4_5_6_7_10_11_12_variable_X_velocity.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 4 Y velocity\n",
      "Saving to \n",
      "concatenated_analyses_data_M4_sessions_1_2_3_4_5_6_7_8_9_10_days_1_2_3_4_5_6_7_10_11_12_variable_Y_velocity.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 4 Head direction\n",
      "Saving to \n",
      "concatenated_analyses_data_M4_sessions_1_2_3_4_5_6_7_8_9_10_days_1_2_3_4_5_6_7_10_11_12_variable_Head_direction.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 5 X position\n",
      "Saving to \n",
      "concatenated_analyses_data_M5_sessions_6_7_8_9_10_11_12_days_7_8_9_10_11_12_13_variable_X_position.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 5 Y position\n",
      "Saving to \n",
      "concatenated_analyses_data_M5_sessions_6_7_8_9_10_11_12_days_7_8_9_10_11_12_13_variable_Y_position.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 5 X velocity\n",
      "Saving to \n",
      "concatenated_analyses_data_M5_sessions_6_7_8_9_10_11_12_days_7_8_9_10_11_12_13_variable_X_velocity.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 5 Y velocity\n",
      "Saving to \n",
      "concatenated_analyses_data_M5_sessions_6_7_8_9_10_11_12_days_7_8_9_10_11_12_13_variable_Y_velocity.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n",
      "M 5 Head direction\n",
      "Saving to \n",
      "concatenated_analyses_data_M5_sessions_6_7_8_9_10_11_12_days_7_8_9_10_11_12_13_variable_Head_direction.mat \n",
      "in folder ./datafiles/concatenated_analyses_processed_extracted_features/\n"
     ]
    }
   ],
   "source": [
    "savedr = './datafiles/concatenated_analyses_processed_extracted_features/'\n",
    "ensure_dir(savedr)\n",
    "cached = {}\n",
    "\n",
    "for animal, sessions in use:\n",
    "    for predict in kininfo.keys():\n",
    "        print('M',animal, kininfo[predict]['name'])\n",
    "        X,Y    = get_data_constrained_analysis_2(animal,sessions,predict,\n",
    "                                                 permute=False,\n",
    "                                                 split=1,\n",
    "                                                 do_add_constant=False)\n",
    "        \n",
    "        cached[animal,tuple(sessions),predict] = (X,Y)\n",
    "        kname  = kininfo[predict]['name'].replace(' ','_')\n",
    "        ss     = '_'.join(map(str,sessions))\n",
    "        daymap = dict(zip(get_session_ids(animal),get_days(animal)))\n",
    "        days   = array([daymap[s] for s in sessions])\n",
    "        dd     = '_'.join(map(str,days))\n",
    "        saveas = 'concatenated_analyses_data_M%d_sessions_%s_days_%s_variable_%s.mat'%(animal,ss,dd,kname)\n",
    "        print('Saving to','\\n'+saveas,'\\nin folder',savedr)\n",
    "        \n",
    "        # Mathfiles are weird this is a hack patch\n",
    "        # TODO: this properly later\n",
    "        savemat(savedr+saveas,{'X':X,'Y':Y})\n",
    "        data   = loadmat(savedr+saveas)\n",
    "        x      = data['X'].squeeze()\n",
    "        y      = data['Y'].squeeze()\n",
    "        x      = array([xi[0] for xi in x])\n",
    "        y      = array([array([yii.T for yii in yi[0]]) for yi in y])\n",
    "        savemat(savedr+saveas,{'X':x,'Y':y})\n",
    "        \n",
    "        # print a summary for sanity checks\n",
    "        nsamples = [cat(yi).shape[0] for yi in y]\n",
    "        #print('Samples per session',nsamples)\n",
    "        #print('Trials per sessions',[yi.shape[0] for yi in y])\n",
    "        #print('%d neurons'%x[0][0].shape[1])\n",
    "        #print('Average samples per session',int(.5+mean(nsamples)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: 1 [1, 4, 5, 6, 7, 10, 14]\n",
      "Passed: 3 [1, 2, 4, 6, 7, 8, 9, 10, 11, 12]\n",
      "Passed: 4 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Passed: 5 [6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "# Verify that X is the same for all kinematic variables\n",
    "for animal, sessions in use:\n",
    "    XX = []\n",
    "    for predict in kininfo.keys():\n",
    "        XX.append(cached[animal,tuple(sessions),predict][0])\n",
    "    ns = len(XX[0])\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            for n in range(ns):\n",
    "                ntr = len(XX[0][n])\n",
    "                for r in range(ntr):\n",
    "                    assert(mean(abs(XX[i][n][r]-XX[j][n][r])) < 1e-9)\n",
    "    print('Passed:',animal,sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: 1 [1, 4, 5, 6, 7, 10, 14]\n",
      "Passed: 3 [1, 2, 4, 6, 7, 8, 9, 10, 11, 12]\n",
      "Passed: 4 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Passed: 5 [6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "# Verify that X is the same for all kinematic variables\n",
    "cached2 = {}\n",
    "for animal, sessions in use:\n",
    "    XX = []\n",
    "    for predict in kininfo.keys():\n",
    "        kname  = kininfo[predict]['name'].replace(' ','_')\n",
    "        ss     = '_'.join(map(str,sessions))\n",
    "        daymap = dict(zip(get_session_ids(animal),get_days(animal)))\n",
    "        days   = array([daymap[s] for s in sessions])\n",
    "        dd     = '_'.join(map(str,days))\n",
    "        saveas = './concatenated_analyses_data_M%d_sessions_%s_days_%s_variable_%s.mat'%(animal,ss,dd,kname)\n",
    "        data   = loadmat(savedr+saveas)\n",
    "        x      = data['X'].squeeze()\n",
    "        y      = data['Y'].squeeze()\n",
    "        XX.append(x)\n",
    "    ns = len(XX[0])\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            for n in range(ns):\n",
    "                ntr = XX[0][n].shape[1]\n",
    "                for r in range(ntr):\n",
    "                    assert(mean(abs(XX[i][n][0,r]-XX[j][n][0,r])) < 1e-9)\n",
    "    print('Passed:',animal,sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair shuffle tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just pair shuffle\n",
    "@memoize\n",
    "def do_pairshuffle(animal,sessions,predict,\n",
    "                                     NXVAL=10,\n",
    "                                     REPL=100,\n",
    "                                     NGRID=20,\n",
    "                                     errtype='L1',\n",
    "                                     matched=False):\n",
    "    '''\n",
    "    Shuffle tests with just two trials\n",
    "    '''\n",
    "    animal = animal # weird cache stuff; ignore but don't remove this line\n",
    "    reset_pool()\n",
    "    jobs = [\n",
    "        (animal,sessions,predict,NXVAL,1,NGRID,errtype,matched,'pair') \n",
    "        for i in range(REPL)\n",
    "        ]\n",
    "    pairshuffle = parmap(constrained_analysis_helper,enumerate(jobs),debug=DEBUG_ON)\n",
    "    print('done')\n",
    "    return pairshuffle\n",
    "\n",
    "# Just pair shuffle\n",
    "@memoize\n",
    "def do_pairshuffle_v2(animal,sessions,predict,\n",
    "                                     NXVAL=10,\n",
    "                                     REPL=100,\n",
    "                                     NGRID=20,\n",
    "                                     errtype='L1',\n",
    "                                     matched=False):\n",
    "    '''\n",
    "    '''\n",
    "    animal = animal # weird cache stuff; ignore but don't remove this line\n",
    "    reset_pool()\n",
    "    jobs = [\n",
    "        (animal,sessions,predict,NXVAL,1,NGRID,errtype,matched,'pair') \n",
    "        for i in range(REPL)\n",
    "        ]\n",
    "    pairshuffle = parmap(constrained_analysis_helper,enumerate(jobs),debug=DEBUG_ON)\n",
    "    print('done')\n",
    "    return pairshuffle\n",
    "\n",
    "def check_basic_same_day_models(x,y,NGRID=20):\n",
    "    x = list(map(np.array,x))\n",
    "    y = list(map(np.array,y))\n",
    "    ww0 = array([reglstsq(xi,yi) for (xi,yi) in zip(x,y)])[:,:,0]\n",
    "    return ww0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get decoding chance level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize\n",
    "def get_chance_level(animal,sessions,predict,\n",
    "    REPL     = 100,\n",
    "    NXVAL    = 10,\n",
    "    errtype  = 'L1',\n",
    "    matched  = True):\n",
    "\n",
    "    all_results = []\n",
    "    print('Computing chance level')\n",
    "    for iteration in progress_bar(range(REPL)):\n",
    "        # This will get the neural and kinematic data for all sessions\n",
    "        X,Y = get_data_constrained_analysis_2(animal,sessions,predict,False,1)\n",
    "\n",
    "        # Now split into 10-fold training/testing sets\n",
    "        errmethod = (errtype+'_degrees') if predict==4 else errtype\n",
    "        efn = neurotools.stats.error_functions[errmethod]\n",
    "        X   = [array(x) for x in X]\n",
    "        Y   = [array(y) for y in Y]\n",
    "        D   = len(X)\n",
    "        N   = len(cat(X))\n",
    "        M   = N/D**2/NXVAL\n",
    "        def partition(x,y):\n",
    "            K      = len(x)\n",
    "            trials = arange(K)\n",
    "            Ntest  = int(M           if matched else K/NXVAL)\n",
    "            Ntrain = int(M*(NXVAL-1) if matched else K-Ntest)\n",
    "            train  = int32(np.random.choice(trials,Ntrain,replace=False))\n",
    "            test   = int32(np.random.choice(list(set(trials)-set(train)),Ntest,replace=False))\n",
    "            return x[train],y[train],x[test],y[test]\n",
    "        trnX,trnY,tstX,tstY = [amap(cat,v) for v in zip(*[partition(x,y) for (x,y) in zip(X,Y)])]\n",
    "\n",
    "        # Shuffle Kinematics\n",
    "        trnY = [block_shuffle(trny,50) for trny in trnY]\n",
    "        tstY = [block_shuffle(tsty,50) for tsty in tstY]\n",
    "\n",
    "        # Now we build models on each training set\n",
    "        ww0 = check_basic_same_day_models(trnX,trnY,NGRID)\n",
    "\n",
    "        # Check model performance\n",
    "        results = [(0,ww0)]\n",
    "        all_results.append(([{'MAW'  :mean(abs(w)),\n",
    "                 'RMSW' :mean(abs(w)**2)**0.5,\n",
    "                 'MADW' :mean(abs(diff(w,axis=0))),\n",
    "                 'RMSDW':mean(abs(diff(w,axis=0))**2)**0.5,\n",
    "                 'MAY'  :mean([mean(abs(y-mean(y))) for y in tstY]), \n",
    "                 'MAE'  :mean([mean(abs(y-x@w)) for (w,x,y) in zip(w,tstX,tstY)]),\n",
    "                 'RMSE' :mean([mean((y-x@w)**2) for (w,x,y) in zip(w,tstX,tstY)])**0.5,\n",
    "                 'MERR' :mean([efn(y,x@w)       for (w,x,y) in zip(w,tstX,tstY)])} \n",
    "                for (l,w) in results],results))\n",
    "    return all_results\n",
    "\n",
    "def get_chance_level_v2(animal,sessions,predict,\n",
    "    NXVAL    = 10,\n",
    "    errtype  = 'L1',\n",
    "    matched  = True):\n",
    "\n",
    "    # This will get the neural and kinematic data for all sessions\n",
    "    X,Y = get_data_constrained_analysis_2(animal,sessions,predict,False,1)\n",
    "\n",
    "    # Now split into 10-fold training/testing sets\n",
    "    errmethod = (errtype+'_degrees') if predict==4 else errtype\n",
    "    efn = neurotools.stats.error_functions[errmethod]\n",
    "    X   = [array(x) for x in X]\n",
    "    Y   = [array(y) for y in Y]\n",
    "    D   = len(X)\n",
    "    N   = len(cat(X))\n",
    "    M   = N/D**2/NXVAL\n",
    "    def partition(x,y):\n",
    "        K      = len(x)\n",
    "        trials = arange(K)\n",
    "        Ntest  = int(M           if matched else K/NXVAL)\n",
    "        Ntrain = int(M*(NXVAL-1) if matched else K-Ntest)\n",
    "        train  = int32(np.random.choice(trials,Ntrain,replace=False))\n",
    "        test   = int32(np.random.choice(list(set(trials)-set(train)),Ntest,replace=False))\n",
    "        return x[train],y[train],x[test],y[test]\n",
    "    trnX,trnY,tstX,tstY = [amap(cat,v) for v in zip(*[partition(x,y) for (x,y) in zip(X,Y)])]\n",
    "\n",
    "    # Shuffle Kinematics\n",
    "    trnY = [block_shuffle(trny,50) for trny in trnY]\n",
    "    tstY = [block_shuffle(tsty,50) for tsty in tstY]\n",
    "\n",
    "    # Now we build models on each training set\n",
    "    ww0 = check_basic_same_day_models(trnX,trnY,NGRID)\n",
    "\n",
    "    # Check model performance\n",
    "    results = [(0,ww0)]\n",
    "    return ([{'MAW'  :mean(abs(w)),\n",
    "             'RMSW' :mean(abs(w)**2)**0.5,\n",
    "             'MADW' :mean(abs(diff(w,axis=0))),\n",
    "             'RMSDW':mean(abs(diff(w,axis=0))**2)**0.5,\n",
    "             'MAY'  :mean([mean(abs(y-mean(y))) for y in tstY]), \n",
    "             'MAE'  :mean([mean(abs(y-x@w)) for (w,x,y) in zip(w,tstX,tstY)]),\n",
    "             'RMSE' :mean([mean((y-x@w)**2) for (w,x,y) in zip(w,tstX,tstY)])**0.5,\n",
    "             'MERR' :mean([efn(y,x@w)       for (w,x,y) in zip(w,tstX,tstY)])} \n",
    "            for (l,w) in results],results)\n",
    "\n",
    "def get_chance_level_helper(p):\n",
    "    i,(animal,sessions,predict,NXVAL,errtype,matched) = p\n",
    "    return i,get_chance_level_v2(animal,sessions,predict,NXVAL,errtype,matched)\n",
    "reset_pool()\n",
    "\n",
    "@memoize\n",
    "def get_chance_level_parallel(animal,sessions,predict,\n",
    "    REPL     = 100,\n",
    "    NXVAL    = 10,\n",
    "    errtype  = 'L1',\n",
    "    matched  = True):\n",
    "    print('Computing chance level (parallel)')\n",
    "    jobs = [(animal,sessions,predict,NXVAL,errtype,matched) for i in range(REPL)]\n",
    "    results = parmap(get_chance_level_helper,enumerate(jobs),debug=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precompute and preload results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1 Kinematics 1 Sessions [1, 4, 5, 6, 7, 10, 14]\n",
      "[##################################################]100.0% \n",
      "done\n",
      "Computing chance level (parallel)\n",
      "[##################################################]100.0% \n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1 Kinematics 3 Sessions [1, 4, 5, 6, 7, 10, 14]\n",
      "[##################################################]100.0% \n",
      "done\n",
      "Computing chance level (parallel)\n",
      "[##################################################]100.0% \n",
      "Subject 1 Kinematics 4 Sessions [1, 4, 5, 6, 7, 10, 14]\n",
      "[##################################################]100.0% \n",
      "done\n",
      "Computing chance level (parallel)\n",
      "[##################################################]100.0% \n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 3 Kinematics 1 Sessions [1, 2, 4, 6, 7, 8, 9, 10, 11, 12]\n",
      "[##################################################]100.0% \n",
      "done\n",
      "Computing chance level (parallel)\n",
      "[##################################################]100.0% \n",
      "Subject 3 Kinematics 3 Sessions [1, 2, 4, 6, 7, 8, 9, 10, 11, 12]\n",
      "[##################################################]100.0% \n",
      "done\n",
      "Computing chance level (parallel)\n",
      "[##################################################]100.0% \n",
      "Subject 3 Kinematics 4 Sessions [1, 2, 4, 6, 7, 8, 9, 10, 11, 12]\n",
      "[##################################################]100.0% \n",
      "done\n",
      "Computing chance level (parallel)\n",
      "[##################################################]100.0% \n",
      "Subject 4 Kinematics 1 Sessions [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[##################################################]100.0% \n",
      "done\n",
      "Computing chance level (parallel)\n",
      "[##################################################]100.0% \n",
      "Subject 4 Kinematics 3 Sessions [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[##################################################]100.0% \n",
      "done\n",
      "Computing chance level (parallel)\n",
      "[##################################################]100.0% \n",
      "Subject 4 Kinematics 4 Sessions [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[#######################                           ] 46.4% "
     ]
    }
   ],
   "source": [
    "# Precompute the results\n",
    "animal,sessions,predict = 4,(1, 2, 3, 4, 5, 6, 7, 8, 9, 10),1\n",
    "results,shuffle,pairshuffle = do_parallel_constrained_analysis(animal,sessions,predict)\n",
    "#pairshuffle = do_pairshuffle(animal,sessions,predict)\n",
    "\n",
    "oldset = {(1,(1, 4, 5, 6, 7, 10, 14),1),\n",
    "    (1,(1, 4, 5, 6, 7, 10, 14),3),\n",
    "    (1,(1, 4, 5, 6, 7, 10, 14),4),\n",
    "    (3,(1, 2, 4, 6, 7, 8, 9, 10, 11, 12),1),\n",
    "    (3,(1, 2, 4, 6, 7, 8, 9, 10, 11, 12),3),\n",
    "    (3,(1, 2, 4, 6, 7, 8, 9, 10, 11, 12),4)}\n",
    "\n",
    "REPL = 500\n",
    "\n",
    "def get_all_data(animal,sessions,predict):\n",
    "    sessions = tuple(sessions)\n",
    "    results,shuffle,pairshuffle = do_parallel_constrained_analysis(animal,sessions,predict,REPL=REPL)\n",
    "    chance = get_chance_level_parallel(animal,sessions,predict,REPL=REPL)\n",
    "    if len(chance)==2:\n",
    "        chance = chance[1]\n",
    "        chance = [ci[0] for ci in chance]\n",
    "    else:\n",
    "        chance = [ci[0][0] for ci in chance]\n",
    "    return results,shuffle,pairshuffle,chance\n",
    "\n",
    "for animal, sessions in use:\n",
    "    for predict in [1,3,4]:\n",
    "        print('Subject',animal,'Kinematics',predict,'Sessions',sessions)\n",
    "        get_all_data(animal,sessions,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks on loaded results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for animal, sessions in use:\n",
    "    for kinematics_predict in [1,3,4]:\n",
    "        print('Subject',animal,'Kinematics',kinematics_predict,'Sessions',sessions)\n",
    "        r,s,p,c = get_all_data(animal,sessions,predict)\n",
    "        print(amap(len,(r,s,p,c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 4B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.dpi']=120\n",
    "\n",
    "def constrained_plot_percentage_v2(animal,sessions,predict):\n",
    "    results,shuffle,pairshuffle,ch = get_all_data(animal,sessions,predict)\n",
    "\n",
    "    # Get prediction errors\n",
    "    r2    = array(list(zip(*results)))\n",
    "    mae   = array([[r['MERR'] for r in rr] for rr in r2])\n",
    "    maw   = array([[r['MAW' ] for r in rr] for rr in r2])\n",
    "    madw  = array([[r['MADW'] for r in rr] for rr in r2])\n",
    "    dwpct = median(madw,axis=1)/median(maw,axis=1)*100\n",
    "    mmae  = median(mae,axis=1)\n",
    "    \n",
    "    # Get shuffle control results\n",
    "    s2     = array(list(zip(*shuffle)))\n",
    "    smae   = array([[r['MERR'] for r in rr] for rr in s2])\n",
    "    smaw   = array([[r['MAW' ] for r in rr] for rr in s2])\n",
    "    smadw  = array([[r['MADW'] for r in rr] for rr in s2])\n",
    "    sdwpct = median(smadw,axis=1)/median(smaw,axis=1)*100\n",
    "    smmae  = median(smae,axis=1)\n",
    "    shuffled = smae[-2,:]\n",
    "    \n",
    "    # Get pair-only shuffle control results\n",
    "    ps2     = array(list(zip(*pairshuffle)))\n",
    "    psmae   = array([[r['MERR'] for r in rr] for rr in ps2])\n",
    "    psmaw   = array([[r['MAW' ] for r in rr] for rr in ps2])\n",
    "    psmadw  = array([[r['MADW'] for r in rr] for rr in ps2])\n",
    "    psdwpct = median(psmadw,axis=1)/median(psmaw,axis=1)*100\n",
    "    psmmae  = median(psmae,axis=1)\n",
    "    pshuffled = psmae[-2,:]\n",
    "    \n",
    "    # Get chance level \n",
    "    chae   = array([r['MERR'] for r in ch])\n",
    "    chance_percent_normalize = mean(chae)/100*2\n",
    "    \n",
    "    # Convert units to percent of chance error\n",
    "    mae  = mae /chance_percent_normalize\n",
    "    smae = smae/chance_percent_normalize\n",
    "    mmae = mmae/chance_percent_normalize\n",
    "    \n",
    "    # Draw |e| vs Δw plot, with endpoints and dashed lines to axis\n",
    "    lw=2\n",
    "    plot(dwpct,mmae,color=BLACK,lw=lw)\n",
    "    scatter([dwpct[0],dwpct[-1]],[mmae[0],mmae[-1]],marker='o',s=8,color=BLACK)\n",
    "\n",
    "    scale = np.max(dwpct)/100\n",
    "    widthscale = 10\n",
    "    bw = widthscale*scale\n",
    "    blw = 1.5\n",
    "    \n",
    "    # Draw concatenated error box\n",
    "    colored_boxplot([mae[-2,:]],[-15*scale],TURQUOISE,widths=bw,zorder=100,linewidth=blw)\n",
    "\n",
    "    # Draw same-day error box\n",
    "    x2 = np.max(dwpct)+15*scale\n",
    "    colored_boxplot([mae[1,:]],[x2],OCHRE,widths=bw,zorder=100,linewidth=blw)\n",
    "    \n",
    "    simpleraxis()\n",
    "    xlabel(r'$\\left<|\\Delta w|\\right>$/session (%)')\n",
    "    xlim(-27*scale,x2+10*scale)\n",
    "    \n",
    "    shuffled = smae[-2,:]\n",
    "    span1   = np.max(shuffled) -np.min(shuffled)\n",
    "    scenter = (np.max(shuffled)+np.min(shuffled))*0.5\n",
    "    \n",
    "    yl = ylim()\n",
    "    more_yticks()\n",
    "    ylim(yl[0],ylim()[1])\n",
    "    plot([dwpct[0]]*2 ,[ylim()[0],mmae[0]] ,color=BLACK,linestyle=':',lw=1)\n",
    "    plot([dwpct[-1]]*2,[ylim()[0],mmae[-1]],color=BLACK,linestyle=':',lw=1)\n",
    "    ylim(yl[0],ylim()[1])\n",
    "    xt = [0,10,12,15,25,30,50,60,np.max(dwpct)]\n",
    "    xticks(xt,['%d'%i for i in xt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(4.1,2.2))\n",
    "\n",
    "animal = 3\n",
    "sessions = [1,  2,  4,  6,  7,  8,  9, 10]\n",
    "ax1 = subplot2grid((2,3),(0,0))\n",
    "ax2 = subplot2grid((2,3),(0,1))\n",
    "ax3 = subplot2grid((2,3),(0,2))\n",
    "sca(ax1); constrained_plot(animal,sessions,1); xlabel('')\n",
    "sca(ax2); constrained_plot(animal,sessions,3); xlabel('')\n",
    "sca(ax3); constrained_plot(animal,sessions,4); xlabel('')\n",
    "\n",
    "animal = 4\n",
    "sessions = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "ax4 = subplot2grid((2,3),(1,0))\n",
    "ax5 = subplot2grid((2,3),(1,1))\n",
    "ax6 = subplot2grid((2,3),(1,2))\n",
    "sca(ax4); constrained_plot(animal,sessions,1); \n",
    "sca(ax5); constrained_plot(animal,sessions,3); ylabel('')\n",
    "sca(ax6); constrained_plot(animal,sessions,4); ylabel('')\n",
    "\n",
    "subplots_adjust(hspace=0.45,wspace=0.5)\n",
    "for i,predict in enumerate([1,3,4]):\n",
    "    sca([ax1,ax2,ax3][i])\n",
    "    title('%s'%kininfo[predict]['name'])\n",
    "\n",
    "savefigure('constrained_sweep_F3B',stamp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U tests\n",
    "\n",
    "Tests whether the concatenated decoders are statistically significantly worse than the same-day decoders. \n",
    "Also test whether the shuffled predictions are significantly better than chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_pvalue(a,b,effective_replicas=1,nsample=1000):\n",
    "    '''\n",
    "    Test if a is larger than b\n",
    "    '''\n",
    "    a,b = amap(ravel,[a,b])\n",
    "    if effective_replicas==1:\n",
    "        d = a[None,:]-b[:,None]\n",
    "        return mean(d<0)\n",
    "    # this isn't quite right but it's pretty close\n",
    "    # ... will revisit later\n",
    "    aa = [mean(choice(a,effective_replicas)) for i in range(nsample)]\n",
    "    bb = [mean(choice(b,effective_replicas)) for i in range(nsample)]\n",
    "    a,b = amap(ravel,[aa,bb])\n",
    "    d = a[None,:]-b[:,None]\n",
    "    return mean(d<0)\n",
    "\n",
    "pvs = {}\n",
    "for animal,sessions in use:\n",
    "    for predict in [1,3,4]:\n",
    "        print('Subject',animal,'Kinematics',kinematics_predict,'Sessions',sessions)\n",
    "        results,shuffle,pairshuffle,ch = get_all_data(animal,sessions,predict)\n",
    "        # Get prediction errors\n",
    "        r2    = array(list(zip(*results)))\n",
    "        mae   = array([[r['MERR'] for r in rr] for rr in r2])\n",
    "        maw   = array([[r['MAW' ] for r in rr] for rr in r2])\n",
    "        madw  = array([[r['MADW'] for r in rr] for rr in r2])\n",
    "        dwpct = median(madw,axis=1)/median(maw,axis=1)*100\n",
    "        mmae  = median(mae,axis=1)\n",
    "        # Get shuffle control results\n",
    "        s2     = array(list(zip(*shuffle)))\n",
    "        smae   = array([[r['MERR'] for r in rr] for rr in s2])\n",
    "        smaw   = array([[r['MAW' ] for r in rr] for rr in s2])\n",
    "        smadw  = array([[r['MADW'] for r in rr] for rr in s2])\n",
    "        sdwpct = median(smadw,axis=1)/median(smaw,axis=1)*100\n",
    "        smmae  = median(smae,axis=1)\n",
    "        shuffled = smae[-2,:]\n",
    "        # Get pair-only shuffle control results\n",
    "        ps2     = array(list(zip(*pairshuffle)))\n",
    "        psmae   = array([[r['MERR'] for r in rr] for rr in ps2])\n",
    "        psmaw   = array([[r['MAW' ] for r in rr] for rr in ps2])\n",
    "        psmadw  = array([[r['MADW'] for r in rr] for rr in ps2])\n",
    "        psdwpct = median(psmadw,axis=1)/median(psmaw,axis=1)*100\n",
    "        psmmae  = median(psmae,axis=1)\n",
    "        pshuffled = psmae[-2,:]\n",
    "        # Get chance level \n",
    "        chae   = array([r['MERR'] for r in ch])\n",
    "        # Compare single-day and concatenated\n",
    "        concat    = mae[-2,:]\n",
    "        singleday = mae[1,:]\n",
    "        pvs[animal,tuple(sessions),predict,'cat_1day']=bootstrap_pvalue(concat,singleday,effective_replicas=len(sessions))\n",
    "        # Get shuffle control results\n",
    "        shuffled = smae[-2,:]\n",
    "        chance   = chae\n",
    "        pvs[animal,tuple(sessions),predict,'shuf_ch']=bootstrap_pvalue(chance,shuffled)\n",
    "        # Get pair shuffle null shuffle control results\n",
    "        paireshuff = psmae[-2,:]\n",
    "        pvs[animal,tuple(sessions),predict,'cat_pair']=bootstrap_pvalue(paireshuff,concat)\n",
    "        \n",
    "from neurotools.stats import pvalues\n",
    "pvs = neurotools.stats.pvalues.correct_pvalues(pvs,True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.dpi']=120\n",
    "\n",
    "def baselegend(*args,**kwargs):\n",
    "    '''\n",
    "    Legend outside the plot on the baes.\n",
    "    '''\n",
    "    defaults = {\n",
    "        'loc':'upper center',\n",
    "        'bbox_to_anchor':(0.5,-0.2),\n",
    "        }\n",
    "    defaults.update(kwargs)\n",
    "    lg = legend(*args,**defaults)\n",
    "    lg.get_frame().set_linewidth(0.0)\n",
    "    return lg\n",
    "\n",
    "def colored_ballplot(datasets,positions,color,size=6,widths=0.8,zorder=100,linewidth=0.8,clip_on=False,**kwargs):\n",
    "    for d,x in zip(datasets,positions):\n",
    "        p5 = percentile(d,5)\n",
    "        p95 = percentile(d,95)\n",
    "        plot([x,x],[p5,p95],color=color,linewidth=linewidth,clip_on=clip_on,**kwargs)\n",
    "        x0 = x-widths/2\n",
    "        x1 = x+widths/2\n",
    "        plot([x0,x1],[p5,p5],color=color,linewidth=linewidth,clip_on=clip_on,**kwargs)\n",
    "        plot([x0,x1],[p95,p95],color=color,linewidth=linewidth,clip_on=clip_on,**kwargs)\n",
    "    for d,x in zip(datasets,positions):\n",
    "        scatter([x],[median(d)],color=color,s=size,clip_on=clip_on,**kwargs)\n",
    "\n",
    "def boxplot_summary(animal,sessions,predict,offset=0,lw=0.8):\n",
    "    results,shuffle,pairshuffle,ch = get_all_data(animal,sessions,predict)\n",
    "\n",
    "    # Get prediction errors\n",
    "    r2    = array(list(zip(*results)))\n",
    "    mae   = array([[r['MERR'] for r in rr] for rr in r2])\n",
    "    maw   = array([[r['MAW' ] for r in rr] for rr in r2])\n",
    "    madw  = array([[r['MADW'] for r in rr] for rr in r2])\n",
    "    dwpct = median(madw,axis=1)/median(maw,axis=1)*100\n",
    "    mmae  = median(mae,axis=1)\n",
    "    \n",
    "    # Get shuffle control results\n",
    "    s2     = array(list(zip(*shuffle)))\n",
    "    smae   = array([[r['MERR'] for r in rr] for rr in s2])\n",
    "    smaw   = array([[r['MAW' ] for r in rr] for rr in s2])\n",
    "    smadw  = array([[r['MADW'] for r in rr] for rr in s2])\n",
    "    sdwpct = median(smadw,axis=1)/median(smaw,axis=1)*100\n",
    "    smmae  = median(smae,axis=1)\n",
    "    shuffled = smae[-2,:]\n",
    "    \n",
    "    # Get pair-only shuffle control results\n",
    "    ps2     = array(list(zip(*pairshuffle)))\n",
    "    psmae   = array([[r['MERR'] for r in rr] for rr in ps2])\n",
    "    psmaw   = array([[r['MAW' ] for r in rr] for rr in ps2])\n",
    "    psmadw  = array([[r['MADW'] for r in rr] for rr in ps2])\n",
    "    psdwpct = median(psmadw,axis=1)/median(psmaw,axis=1)*100\n",
    "    psmmae  = median(psmae,axis=1)\n",
    "    pshuffled = psmae[-2,:]\n",
    "    \n",
    "    # Get chance level \n",
    "    chae   = array([r['MERR'] for r in ch])\n",
    "    baseline = mean(chae)/100\n",
    "    \n",
    "    width = 0.75\n",
    "    # Draw concatenated error box\n",
    "    colored_ballplot([mae[-2,:]/baseline],[3+offset],TURQUOISE,widths=width,zorder=100,linewidth=lw)\n",
    "    # Draw same-day error box\n",
    "    colored_ballplot([mae[1,:]/baseline],[4+offset],OCHRE,widths=width,zorder=100,linewidth=lw)\n",
    "    # Set axis limits\n",
    "    simpleraxis()\n",
    "    ylabel(kininfo[predicted_variable]['name']+' error (%s)'%kininfo[predicted_variable]['units'])\n",
    "    # Shuffle concatenated error box\n",
    "    colored_ballplot([shuffled/baseline],[1+offset],RUST,widths=width,zorder=100,linewidth=lw)\n",
    "    # Pair-shuffle concatenated error box\n",
    "    colored_ballplot([pshuffled/baseline],[2+offset],MAUVE,widths=width,zorder=100,linewidth=lw)\n",
    "    \n",
    "    if predict==1:\n",
    "        ylim(25,80)\n",
    "    if predict==3:\n",
    "        ylim(30,90)\n",
    "    if predict==4:\n",
    "        ylim(40,100)\n",
    "    xticks([])\n",
    "\n",
    "\n",
    "figure(figsize=(TEXTWIDTH,TEXTWIDTH/3))\n",
    "SPACING = 5\n",
    "fudge = 0.15\n",
    "for iplot,predicted_variable in enumerate([1,3,4]):\n",
    "    subplot(1,3,iplot+1)\n",
    "    for iu,(a,ss) in enumerate(use):\n",
    "        boxplot_summary(a,ss,predicted_variable,iu*SPACING)\n",
    "    ylabel('% chance-level error' if iplot==0 else '')\n",
    "    title(kininfo[predicted_variable]['name'],fontsize=10)\n",
    "    xlim(-0.2,xlim()[1])\n",
    "    xticks(arange(4)*SPACING+2.5+fudge,['M%d'%i for i in [1,3,4,5]])\n",
    "    gca().tick_params(axis='x',length=0)\n",
    "    axvspan(SPACING*1+fudge,SPACING*2+fudge,color=WHITE,zorder=-inf)\n",
    "    axvspan(SPACING*3+fudge,SPACING*4+fudge,color=WHITE,zorder=-inf)\n",
    "    xlim(0+fudge,SPACING*4+fudge)\n",
    "\n",
    "tight_layout()\n",
    "subplot(132)\n",
    "xl = xlim()\n",
    "yl = ylim()\n",
    "scatter(-1000,10,s=60,marker='s',color=RUST     ,label='Permute (all)')\n",
    "scatter(-1000,10,s=60,marker='s',color=MAUVE    ,label='Permute (pair)')\n",
    "scatter(-1000,10,s=60,marker='s',color=TURQUOISE,label='Concatenated')\n",
    "scatter(-1000,10,s=60,marker='s',color=OCHRE    ,label='Single-day')\n",
    "xlim(*xl)\n",
    "ylim(*yl)\n",
    "baselegend(handletextpad=0,ncol=4,fontsize=7.5)\n",
    "subplots_adjust(bottom=0.23)\n",
    "savefigure('all_mice_decode_concatenated',stamp=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
