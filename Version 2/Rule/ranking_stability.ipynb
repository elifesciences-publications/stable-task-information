{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell rankings and stability checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data location is /home/mer49/Workspace2/PPC_data/\n",
      "Matplotlib configured\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Add local scripts to path\n",
    "import os,sys\n",
    "sys.path.insert(0,os.path.abspath(\"./\"))\n",
    "import neurotools\n",
    "\n",
    "# Set up cache\n",
    "from neurotools.jobs.initialize_system_cache import initialize_caches,cache_test\n",
    "PYCACHEDIR = os.path.abspath('./')\n",
    "CACHENAME  = 'PPC_cache'\n",
    "from neurotools.tools import ensure_dir\n",
    "ensure_dir(PYCACHEDIR+os.sep+CACHENAME)\n",
    "initialize_caches(\n",
    "    level1  = PYCACHEDIR,\n",
    "    force   = False,\n",
    "    verbose = False,\n",
    "    CACHE_IDENTIFIER = CACHENAME)\n",
    "\n",
    "# Import libraries\n",
    "from neurotools.nlab import *\n",
    "import ppc_data_loader\n",
    "\n",
    "# Set this to the location of the PPC data on your machine\n",
    "ppc_data_loader.path = '/home/mer49/Dropbox (Cambridge University)/Datasets/PPC_data/'\n",
    "from ppc_data_loader   import *\n",
    "from ppc_trial         import *\n",
    "\n",
    "# Configure Numpy\n",
    "np.seterr(all='raise');\n",
    "from numpy.linalg import solve\n",
    "np.random.seed(0)\n",
    "\n",
    "# Configure Matplotlib\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 120\n",
    "TEXTWIDTH = 5.62708\n",
    "matplotlib.rcParams['figure.figsize'] = (TEXTWIDTH, TEXTWIDTH/sqrt(2))\n",
    "import warnings\n",
    "from matplotlib import MatplotlibDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\",category=MatplotlibDeprecationWarning)\n",
    "SMALL_SIZE  = 7\n",
    "MEDIUM_SIZE = 8\n",
    "BIGGER_SIZE = 9\n",
    "matplotlib.rc('font'  , size     =SMALL_SIZE ) # controls default text sizes\n",
    "matplotlib.rc('axes'  , titlesize=MEDIUM_SIZE) # fontsize of the axes title\n",
    "matplotlib.rc('axes'  , labelsize=MEDIUM_SIZE) # fontsize of the x and y labels\n",
    "matplotlib.rc('xtick' , labelsize=SMALL_SIZE ) # fontsize of the tick labels\n",
    "matplotlib.rc('ytick' , labelsize=SMALL_SIZE ) # fontsize of the tick labels\n",
    "matplotlib.rc('legend', fontsize =SMALL_SIZE ) # legend fontsize\n",
    "matplotlib.rc('figure', titlesize=BIGGER_SIZE) # fontsize of the figure title\n",
    "print('Matplotlib configured')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data (just one session for starters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly select subject and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using animal 5\n",
      "Using trials with cued direction right\n",
      "Using trials with previous direction right\n",
      "Using session 1\n",
      "\n",
      "85% of timepoints are within a trial\n",
      "y dimension is 4.580330147041574 meters\n",
      "x dimension is 0.15015015015015015 meters\n",
      "peak dx from position is 0.7997261576589432\n",
      "peak dx from velocity is 0.47088561809043883\n",
      " session_obj:\n",
      " :  | confidenceLabel | 955 x 1 | float64 | 'numpy.ndarray | \n",
      " :  | deltaDays       | 1 x 33  | float64 | 'numpy.ndarray | \n",
      " :  | numConditions   | 1 x 34  | float64 | 'numpy.ndarray | \n",
      " :  | sessionList     | 1 x 33  | object  | 'numpy.ndarray | \n",
      " :  | sessionNumber   | 1 x 1   | float64 | 1.0            | \n",
      " : analysis:\n",
      " : : glm:\n",
      " : : :  | cv_object           | 1 x 955     | object  | 'numpy.ndarray | \n",
      " : : :  | filterMatrix        | 11580 x 144 | float64 | 'numpy.ndarray | \n",
      " : : :  | filterMatrixIDs     | 144 x 1     | object  | 'numpy.ndarray | \n",
      " : : :  | foldID              | 11580 x 1   | float64 | 'numpy.ndarray | \n",
      " : : :  | frame_idx           | 11580 x 1   | float64 | 'numpy.ndarray | \n",
      " : : :  | testFoldGreaterThan | 1 x 1       | float64 | 8.0            | \n",
      " : : peaks:\n",
      " : : :  | meanActivity     | 60 x 955 x 2 | float64 | 'numpy.ndarray | \n",
      " : : :  | significantPeaks | 60 x 955 x 2 | float64 | 'numpy.ndarray | \n",
      " : : stability:\n",
      " : : :  | sorted_cell_ids | 1 x 45 | float64 | 'numpy.ndarray | \n",
      " : : :  | sorted_index    | 1 x 45 | float64 | 'numpy.ndarray | \n",
      " : : :  | sorted_order    | 39 x 1 | uint16  | 'numpy.ndarray | \n",
      " : : svm:\n",
      " : : :  | accuracy   | 20 x 955 | float64 | 'numpy.ndarray | \n",
      " : : :  | dimensions | 46 x 1   | uint16  | 'numpy.ndarray | \n",
      " : : :  | weights    | 20 x 955 | float64 | 'numpy.ndarray | \n",
      " : refImages:\n",
      " : : slice:\n",
      " : : :  | img | 4 x 1 | object | 'numpy.ndarray | \n",
      " : roiInfo:\n",
      " : : slice:\n",
      " : : :  | rawF          | 4 x 1 | object | 'numpy.ndarray | \n",
      " : : :  | roiIndex      | 4 x 1 | object | 'numpy.ndarray | \n",
      " : : :  | traceNeuropil | 4 x 1 | object | 'numpy.ndarray | \n",
      " : timeSeries:\n",
      " : :  | frameRate      | 1 x 1 | float64 | 5.3            | \n",
      " : :  | frameRateUnits | 8 x 1 | uint16  | 'numpy.ndarray | \n",
      " : : calcium:\n",
      " : : :  | data   | 1 x 2 | object | 'numpy.ndarray | \n",
      " : : :  | labels | 1 x 2 | object | 'numpy.ndarray | \n",
      " : : virmen:\n",
      " : : :  | data   | 12000 x 10 | float64 | 'numpy.ndarray | \n",
      " : : :  | labels | 1 x 10     | object  | 'numpy.ndarray | \n",
      " : trials:\n",
      " : :  | correct               | 1 x 133        | float64 | 'numpy.ndarray | \n",
      " : :  | spData                | 955 x 21 x 133 | float64 | 'numpy.ndarray | \n",
      " : :  | spDataSmallBins       | 4 x 1          | object  | 'numpy.ndarray | \n",
      " : :  | spDataSmallBinsLabels | 4 x 1          | object  | 'numpy.ndarray | \n",
      " : :  | trialType             | 1 x 133        | float64 | 'numpy.ndarray | \n",
      " : :  | trialTypeLabels       | 4 x 1          | object  | 'numpy.ndarray | \n",
      " : :  | virmenData            | 5 x 21 x 133   | float64 | 'numpy.ndarray | \n",
      " : :  | virmenDataLabels      | 1 x 5          | object  | 'numpy.ndarray | \n",
      "0 timestamp\n",
      "1 lateral position(m)\n",
      "2 forward position(m)\n",
      "3 view angle (deg)\n",
      "4 lateral speed(m/s)\n",
      "5 forward speed(m/s)\n",
      "6 trial type (black right = 2, white left = 3)\n",
      "7 reward\n",
      "8 inter-trial interval\n",
      "9 cue offset position(m)\n"
     ]
    }
   ],
   "source": [
    "MINNEURONS = 50   # Only consider sessions w. at least this many neurons\n",
    "MAXSAMPLES = 400  # Remove overly-long trials\n",
    "NXVAL      = 10   # No. crossvalidation blocks\n",
    "JITTER     = 25   # No. frames tolerance btwn trial end marker & reward\n",
    "LOWF       = 0.03 # Low-frequency cutoff for filtering\n",
    "HIGHF      = 1.25 # High-frequency cutoff for filtering\n",
    "\n",
    "animals    = get_subject_ids()\n",
    "animal     = choice(animals)\n",
    "cues       = [ppc_trial.Trial.CUE_LEFT,ppc_trial.Trial.CUE_RIGHT]\n",
    "CUE        = choice(cues)\n",
    "PREV       = choice(cues)\n",
    "cname      = ['left','right'][CUE]\n",
    "pname      = ['left','right'][PREV]\n",
    "sessions   = get_session_ids(animal)\n",
    "session    = choice(sessions)\n",
    "units      = good_units_index(animal,session)\n",
    "NNEURONS   = len(units)\n",
    "FS         = get_FS(animal,session)\n",
    "\n",
    "print('Using animal' ,animal )\n",
    "print('Using trials with cued direction',cname)\n",
    "print('Using trials with previous direction',pname)\n",
    "print('Using session',session)\n",
    "\n",
    "#### Sanity check that physical units seem correct\n",
    "intrials = get_intrial(animal,session)\n",
    "print('\\n%02d%% of timepoints are within a trial'%(100*mean(intrials)))\n",
    "print('y dimension is %s meters'%np.max(get_y(animal,session)[intrials]))\n",
    "print('x dimension is %s meters'%np.max(get_x(animal,session)[intrials]))\n",
    "print('peak dx from position is',\\\n",
    "      np.max(abs(diff(get_x(animal,session)))[intrials[:-1]])*FS)\n",
    "print('peak dx from velocity is',\\\n",
    "      np.max(abs(get_dx(animal,session))[intrials]))\n",
    "\n",
    "hdfdata = get_data(animal,session)\n",
    "printmatHDF5(hdfdata)\n",
    "for i,label in enumerate(getHDF(hdfdata,'session_obj/timeSeries/virmen/labels')):\n",
    "    print(i,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Decode position, speed, and angle\n",
    "\n",
    "Position has both real (x,y) location as well a pseudotime location; Which to use? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def get_in_trial(signal,animal,session,MAXSAMPLES=MAXSAMPLES,JITTER=JITTER,meanzero=True,dozscore=False):\n",
    "    # Find edges of all CORRECT trials\n",
    "    all_trials = get_basic_trial_info(animal,session,pad_edges=False,JITTER=JITTER)\n",
    "    use_trials = [t for t in all_trials if t.correct and t.nsample<MAXSAMPLES]\n",
    "    starts     = [t.istart for t in use_trials]\n",
    "    stops      = [t.istop  for t in use_trials]\n",
    "    snips      = [signal[a:b,...] for (a,b) in zip(starts,stops)]\n",
    "    if meanzero: # mean-center each trial separately\n",
    "        snips = list(map(lambda x:zeromean(x,axis=0),snips))\n",
    "    if dozscore:\n",
    "        snips = list(map(lambda x:zscore(x,axis=0),snips))\n",
    "    return np.concatenate(snips)\n",
    "\n",
    "@memoize\n",
    "def get_neural_signals_for_training(animal,session,units=None,verbose=False):\n",
    "    # Get filtered log calcium transients\n",
    "    if units is None: \n",
    "        units = good_units_index(animal,session)\n",
    "    #Y  = array([get_smoothed_dFF(animal,session,u,LOWF,HIGHF) for u in units])\n",
    "    dFF = get_dFF(animal,session,units)\n",
    "    FS  = get_FS(animal,session)\n",
    "    Y   = array([bandpass_filter(z,fa=LOWF,fb=HIGHF,Fs=FS) for z in dFF.T])\n",
    "    if verbose:\n",
    "        print('Sample rate is %f Hz'%get_FS(animal,session))\n",
    "        print('Filtering between %0.2f and %0.2f Hz'%(LOWF,HIGHF))\n",
    "        print('Obtained filtered calcium signals, shape is',Y.shape)\n",
    "    ydata = zeromean(get_in_trial(Y.T,animal,session),axis=0)\n",
    "    times = arange(ydata.shape[0])/FS\n",
    "    return times, ydata\n",
    "\n",
    "def polar_error_degrees(xdata,xhat):\n",
    "    # Report error in physical units\n",
    "    herr = abs(xdata-xhat)\n",
    "    herr[herr>180] = 360-herr[herr>180]\n",
    "    mabs = mean(abs(herr))\n",
    "    rmse = mean(herr**2)**0.5\n",
    "    return rmse, mabs\n",
    "\n",
    "@memoize\n",
    "def head_direction_prediction_error(animal,session,units=None,mode='polar'):\n",
    "    times, ydata = get_neural_signals_for_training(animal,session,units=units)\n",
    "    if mode=='polar':\n",
    "        theta      = get_in_trial(get_theta(animal,session),animal,session,meanzero=False)*pi/180\n",
    "        sincos     = array([sin(theta),cos(theta)]).T\n",
    "        xmean      = mean(sincos,axis=0)\n",
    "        xdata      = zeromean(sincos,axis=0)\n",
    "        W,xhat,_,_ = crossvalidated_least_squares(ydata,xdata,NXVAL)\n",
    "        # Compute and return error measures\n",
    "        thetahat   = angle((xmean+xhat)@[1j,1])\n",
    "        rmse, mabs = polar_error_degrees(theta*180/pi,thetahat*180/pi)\n",
    "        return theta, thetahat, rmse, mabs\n",
    "    elif mode=='linear':\n",
    "        # Angle decoding in degrees\n",
    "        xdata      = get_in_trial(get_theta(animal,session),animal,session,meanzero=True)\n",
    "        xmean      = mean(xdata)\n",
    "        xdata      = zeromean(xdata)\n",
    "        W,xhat,_,_ = crossvalidated_least_squares(ydata,xdata,NXVAL)\n",
    "        # Compute and return error measures\n",
    "        rmse, mabs = polar_error_degrees(xdata,xhat)\n",
    "        return xdata+xmean, xhat+xmean, rmse, mabs\n",
    "    raise ArgumentError('Mode should be either polar or linear')\n",
    "\n",
    "@memoize\n",
    "def speed_prediction_error(animal,session,units=None):\n",
    "    '''\n",
    "    Error in decoding Y speed\n",
    "    '''\n",
    "    times, ydata = get_neural_signals_for_training(animal,session,units=units)\n",
    "    dy    = get_dy(animal,session)\n",
    "    speed = get_in_trial(abs(dy),animal,session)\n",
    "    xdata = zeromean(speed)\n",
    "    xmean = mean(speed)\n",
    "    W,xhat,_,_ = crossvalidated_least_squares(ydata,xdata,NXVAL)\n",
    "    # Compute and return error measures\n",
    "    xerr  = (xdata-xhat)\n",
    "    mabs  = mean(abs(xerr))\n",
    "    rmse  = mean(abs(xerr)**2)**0.5\n",
    "    return times, speed, xhat+xmean, rmse, mabs\n",
    "\n",
    "@memoize\n",
    "def position_prediction_error(animal,session,units=None):\n",
    "    '''\n",
    "    Error in decoding Y position\n",
    "    '''\n",
    "    times, ydata = get_neural_signals_for_training(animal,session,units=units)\n",
    "    y     = get_y(animal,session)\n",
    "    xdata = get_in_trial(y,animal,session)\n",
    "    xmean = mean(xdata,axis=0)\n",
    "    xdata = zeromean(xdata,axis=0)\n",
    "    W,xhat,_,_ = crossvalidated_least_squares(ydata,xdata,NXVAL)\n",
    "    # Compute and return error measures\n",
    "    xerr = zeromean(xdata)-zeromean(xhat)\n",
    "    mabs = mean(abs(xerr))\n",
    "    rmse = mean(abs(xerr)**2)**0.5\n",
    "    return xdata+xmean, xhat+xmean, rmse, mabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy optimization of cross-validated fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify crossvalidated least-squares to precompute covariance structure\n",
    "\n",
    " - Break data into testing and training groups\n",
    " - Get covariances for each training group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed-form L2 error for greedy sorting\n",
    "\n",
    "See personal notebook for derivation. We think the L2 error should be\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\operatorname{tr}\\left(\n",
    "\\Sigma_{b_\\text{test}} + \\Sigma_{ba} \\Sigma_{aa}^{-1}\n",
    "\\Sigma_{aa_\\text{test}}\\Sigma_{aa}^{-1}\\Sigma_{ab}-2\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab_\\text{test}}\n",
    "\\right)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Abbreviated derivation of the code:\n",
    "\n",
    "    mean((tsb - tsa @ solve(aatr,abtr))**2)\n",
    "    mean(tsb**2 + (tsa @ solve(aatr,abtr))**2 - 2*tsb*(tsa @ solve(aatr,abtr)))\n",
    "    mean(tsb**2) + mean((tsa @ solve(aatr,abtr))**2) - 2*mean(tsb*(tsa @ solve(aatr,abtr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron-stability and information correlations in an example datastaset\n",
    "\n",
    "Get\n",
    " - Individual neuron predictions\n",
    " - Neuron predictions in ensemble\n",
    " - Neuron prediction with ascending greedy search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage of cells and composition of informative subsets over time\n",
    "\n",
    "Get the following for each day: \n",
    "\n",
    " - Ascending  Greedy sort with per-cell Δerrors\n",
    " - Descending Greedy sort with per-cell Δerrors\n",
    " - Single cell Δerrors\n",
    " \n",
    "Do this for a range of regularization strengths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution: changing the source code of the cell below will reset any previously cached results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memoize\n",
    "def get_importance_forward_position(animal,session,units,reg,method,NXVAL):\n",
    "    \n",
    "    # Load data\n",
    "    times,ydata = get_neural_signals_for_training(animal,session)\n",
    "    avail_units = good_units_index(animal,session)\n",
    "    xdata       = get_in_trial(get_y(animal,session),animal,session,dozscore=True)\n",
    "    \n",
    "    units = array(sorted(list(set(units))))\n",
    "    assert(set(units)==(set(avail_units)&set(units)))\n",
    "    pick  = array([where(avail_units==u)[0][0] for u in units])\n",
    "    ydata = ydata[:,pick]\n",
    "    u2    = avail_units[pick]\n",
    "    assert(all(u2==units))\n",
    "    \n",
    "    # Error with all cells (best-case)\n",
    "    T,N  = ydata.shape\n",
    "    R    = reg*eye(N)\n",
    "    xhat = crossvalidated_least_squares(ydata,xdata,NXVAL,reg=reg)[1]\n",
    "    err0 = mean((xdata-xhat)**2)\n",
    "    \n",
    "    # Break data into testing and training groups\n",
    "    trA,trB,tsA,tsB = partition_data_for_crossvalidation(ydata,xdata,NXVAL)\n",
    "    \n",
    "    # Get covariances for each training group\n",
    "    # We solve linear least squares as:\n",
    "    # Solves Ax=B for x with L2 regularization\n",
    "    # Q = A.T.dot(A) + np.eye(A.shape[1])*reg*A.shape[0]\n",
    "    # return np.linalg.solve(Q, A.T.dot(B))\n",
    "    sAtr  = [float32((a.T@ a)/a.shape[0]+R) for a     in trA         ] # Train ind.\n",
    "    sABtr = [float32((a.T@ b)/a.shape[0])   for (a,b) in zip(trA,trB)] # Train cross\n",
    "    sBts  = [float32((b.T@ b)/b.shape[0])   for b     in tsB         ] # Test dep.\n",
    "    sAts  = [float32((a.T@ a)/a.shape[0])   for a     in tsA         ] # Test ind. NO REGULARIZATION FOR THIS ONE\n",
    "    sABts = [float32((a.T@ b)/a.shape[0])   for (a,b) in zip(tsA,tsB)] # Test cross\n",
    "    emax  = mean(sBts) # Baseline maximum error rate (variance of testing data)\n",
    "    \n",
    "    def xverrsubset(u):\n",
    "        u = int32(array(sorted(list(set(u)))))\n",
    "        error = emax*NXVAL\n",
    "        for Σaa,Σab,Σaav,Σabv in zip(sAtr,sABtr,sAts,sABts):\n",
    "            w = solve(Σaa[u,:][:,u],Σab[u])\n",
    "            error += w.T @ (Σaav[:,u] @ w - 2 * Σabv)[u]\n",
    "        return error/NXVAL\n",
    "\n",
    "    if method=='ascending_greedy':\n",
    "        unused = set(arange(N))\n",
    "        uids   = []\n",
    "        errs   = []\n",
    "        for i in progress_bar(range(N)):\n",
    "            check   = array(sorted(list(unused)))\n",
    "            er      = [xverrsubset(uids+[ch]) for ch in check]\n",
    "            best    = check[argmin(er)]\n",
    "            uids   += [best]\n",
    "            errs   += [np.min(er)]\n",
    "            unused -= {best}\n",
    "        # Neurons are added to the population by\n",
    "        # greedy search. Each neuron added decreases the\n",
    "        # error (starting at emax) by some amount. We add\n",
    "        # the best neuron on each iteration. The result\n",
    "        # should be the neurons sorted from most to least\n",
    "        # important. \n",
    "        Δerrs = -diff([emax]+errs)\n",
    "        sys.stdout.write('\\r'+' '*70+'\\r'); sys.stdout.flush()\n",
    "    \n",
    "    if method=='descending_greedy':\n",
    "        used   = set(arange(N))\n",
    "        uids   = []\n",
    "        errs   = []\n",
    "        for i in progress_bar(range(N-1)):\n",
    "            check = array(sorted(list(used)))\n",
    "            er    = [xverrsubset(used-{ch}) for ch in check]\n",
    "            best  = check[argmin(er)]\n",
    "            uids += [best]\n",
    "            errs += [np.min(er)]\n",
    "            used -= {best}\n",
    "        last  = list(used)\n",
    "        uids += last\n",
    "        errs += [xverrsubset(last)]\n",
    "        # Neurons are removed from the population, removing\n",
    "        # neurons that don't matter first. The result is\n",
    "        # a list of neurons sorted in increasing order of\n",
    "        # importance. Error steadily increases in this\n",
    "        # procedure.\n",
    "        Δerrs = diff([err0]+errs)\n",
    "        # Switch to descending order of importance\n",
    "        Δerrs = Δerrs[::-1]\n",
    "        errs  =  errs[::-1]\n",
    "        uids  =  uids[::-1]\n",
    "        \n",
    "    if method=='single_cell':\n",
    "        uids  = arange(N)\n",
    "        # Error after ADDING the unit from the population\n",
    "        # This should lower the error relative to guessing\n",
    "        # (emax). The more the error is lowered, the better\n",
    "        # the neuron is\n",
    "        errs  = array([xverrsubset([i]) for i in uids])\n",
    "        Δerrs = emax-errs\n",
    "        order = argsort(-Δerrs)\n",
    "        errs  = errs[order]\n",
    "        uids  = uids[order]\n",
    "        Δerrs = Δerrs[order]\n",
    "\n",
    "    if method=='leave_one_out':\n",
    "        uids  = arange(N)\n",
    "        # Error after REMOVING the unit from the population\n",
    "        # This should increase the error above optimum err0\n",
    "        # The higher this is, the more important neuron was\n",
    "        errs  = array([xverrsubset(set(uids)-{j}) for j in uids])\n",
    "        Δerrs = errs - err0\n",
    "        order = argsort(-Δerrs)\n",
    "        errs  = errs[order]\n",
    "        uids  = uids[order]\n",
    "        Δerrs = Δerrs[order]\n",
    "    \n",
    "    return err0, emax, errs, Δerrs, uids, units[uids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "use = [(1,[ 1,  4,  5,  6,  7, 10, 14]),\n",
    "       (3,[ 1,  2,  4,  6,  7,  8,  9, 10, 11, 12]),\n",
    "       (3,[13, 14, 15, 16, 17, 18, 19, 20, 21, 22]),\n",
    "       (4,[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]),\n",
    "       (5,[ 6,  7,  8,  9, 10, 11, 12])]\n",
    "\n",
    "NXVAL        = 10\n",
    "methods      = ['single_cell','leave_one_out','ascending_greedy','descending_greedy']\n",
    "regstrengths = [1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-8,1e-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1\n",
      "Subject 3                                                             \n",
      "Subject 3                                                             \n",
      "Subject 4                                                             \n",
      "[######                                            ] 12%  18/142"
     ]
    }
   ],
   "source": [
    "RECOMPUTE_RESULTS = False\n",
    "saveto = './datafiles/importance_sorting_result.p'\n",
    "if RECOMPUTE_RESULTS:\n",
    "    results   = {}\n",
    "    timestamp = now()\n",
    "    for animal,sessions in use:\n",
    "        print('Subject',animal)\n",
    "        units,uidxs = get_units_in_common(animal,sessions)\n",
    "        for session in progress_bar(sessions):\n",
    "            for method in methods:\n",
    "                for reg in regstrengths: \n",
    "                    results[animal,session,method,reg] = get_importance_forward_position(animal,session,units,reg,method,NXVAL)\n",
    "                    # err0, emax, errs, Δerrs, uids, units[uids]\n",
    "            pickle.dump(results,open(saveto,'wb'))\n",
    "else:\n",
    "    results = pickle.load(open(saveto,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check situation with unit ID consistency number\n",
    "\n",
    " - 1: same cell\n",
    " - 2: likely the same cell\n",
    " - 3: maybe the same cell\n",
    " - 4: Nuclear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for animal, sessions in use:\n",
    "    units,uidxs = get_units_in_common(animal,sessions)\n",
    "    stability_index = array([get_recording_stability_index(animal,s) for s in sessions])\n",
    "    print(np.min(stability_index,axis=0)[units])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to check? \n",
    "\n",
    " - Pick optimal subset that explains most of the variance\n",
    " - Reproduce these claims in the paper\n",
    "   - 6% of neurons are consistently in the top 50%\n",
    "   - 0% of nerons are in the top 10%\n",
    "   - These check out, results are similar for various levels of regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for method in methods:\n",
    "    print('\\nUsing method:',method)\n",
    "    print('==============================')\n",
    "\n",
    "    for animal,sessions in use:\n",
    "        print('\\nM%d (%2d sessions spanning %2d days)'%(animal,len(sessions),sessions[-1]-sessions[0]))\n",
    "        print('---------------------------------')\n",
    "            \n",
    "        for reg in [1e-10,1e-4,1e-2,1e-1]:\n",
    "            print('Regularization strength %0.0e:'%reg)\n",
    "            top10 = []\n",
    "            top50 = []\n",
    "            errs  = []\n",
    "            err30 = []\n",
    "            pct99 = []\n",
    "            for session in sessions:\n",
    "                err0, emax, errs, Δerrs, uids, units = results[animal,session,method,reg]\n",
    "                Nunits = len(units)\n",
    "                N10    = int(Nunits/10)\n",
    "                N30    = int(Nunits/3)\n",
    "                N50    = int(Nunits/2)\n",
    "                top10 += [units[:N10]]\n",
    "                top50 += [units[:N50]]\n",
    "                err30 += [errs[N30]/emax]\n",
    "                errs  += [err0/emax]\n",
    "                if 'greedy' in method:\n",
    "                    N99    = find((1-errs/emax)>=(1-err0/emax)*0.99)[0]\n",
    "                    pct99 += [N99/Nunits]\n",
    "            all10 = set.intersection(*map(set,top10))\n",
    "            all50 = set.intersection(*map(set,top50))\n",
    "            print(' - Cells always in the top 10%% : %2d units, %2d%%'%(len(all10),len(all10)*100/Nunits))\n",
    "            print(' - Cells always in the top 50%% : %2d units, %2d%%'%(len(all50),len(all50)*100/Nunits))\n",
    "            if 'greedy' in method:\n",
    "                print(' - Average NMSE using all cells: %2d%% ±%2dσ'%(mean(errs)*100,std(errs)*100))\n",
    "                print(' - Average NMSE using top 30%%  : %2d%%'%(100*mean(err30)))\n",
    "                print(' - 30%% of cells account for %d%% of variance explained by full population'%(100*(1-mean(err30))/(1-mean(errs))))\n",
    "                print(' - %2d%% of cells account for 99%% of variance explained by full population'%(100*mean(pct99)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We considered whether it might be possible to identify a long-term stable subset based on single-day decoding performance. To assess this, we ranked subsets of cells using greedy search, based on their contribution to cross-validated decoding within a single session (Methods). On any single session, no more than 30% of cells were needed to achieve 99% of the explained variance of the full population. Despite this, it was impossible to identify a core subset that was stable over time. No more than 1% of cells were ranked in the top 10% for importance on all days, an no more than 13% in the top 50% for all days. \n",
    "\n",
    "We also evaluted whether regularization could affect these results. Regularization was performed by adding a constant time the identity matrix $\\lambda I$ to the covariance matrix of the neural data. For all sessions, we found that generalization error could be slightly reduced using regularization strengths between $\\lambda=10^{-4}$ to $10^{-3}$. Despite this, regularization did not improve our ability to predict which cells would be stable over days. Similar results were obtained for a range of regularization values ranging from $\\lambda=10^{-10}$ to $10^{-1}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for animal, sessions in use:\n",
    "    all_errs = []\n",
    "    for reg in regstrengths[1:-2]:\n",
    "        errs = []\n",
    "        for session in sessions:\n",
    "            err0, emax, errs, Δerrs, uids, units = results[animal,session,method,reg]\n",
    "            errs += [err0/emax]\n",
    "        #print('%0.2e %0.4f'%(reg,mean(errs)))\n",
    "        all_errs += [mean(errs)]\n",
    "\n",
    "    subplot(221)\n",
    "    plot(log10(regstrengths[1:-2]), all_errs/all_errs[-1]*100,lw=1)\n",
    "    simpleraxis()\n",
    "    xlabel('Regularization strength')\n",
    "    ylabel('% change in NMSE')\n",
    "    xticks(range(-6,-1,2),['$10^{%d}$'%d for d in range(-6,-1,2)]);\n",
    "    axhline(100,lw=1,color='k',linestyle=':')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for method in methods:\n",
    "    print('\\nUsing method:',method)\n",
    "    print('==============================')\n",
    "\n",
    "    min10,max10 = inf,-inf\n",
    "    min50,max50 = inf,-inf\n",
    "    min99,max99 = inf,-inf\n",
    "    for animal,sessions in use:\n",
    "        for reg in [1e-10,1e-6,1e-3,1e-2]:\n",
    "            top10 = []\n",
    "            top50 = []\n",
    "            errs  = []\n",
    "            err30 = []\n",
    "            pct99 = []\n",
    "            for session in sessions:\n",
    "                err0, emax, errs, Δerrs, uids, units = results[animal,session,method,reg]\n",
    "                Nunits = len(units)\n",
    "                N10    = int(Nunits/10)\n",
    "                N30    = int(Nunits/3)\n",
    "                N50    = int(Nunits/2)\n",
    "                top10 += [units[:N10]]\n",
    "                top50 += [units[:N50]]\n",
    "                err30 += [errs[N30]/emax]\n",
    "                errs  += [err0/emax]\n",
    "                if 'greedy' in method:\n",
    "                    N99    = find((1-errs/emax)>=(1-err0/emax)*0.99)[0]\n",
    "                    pct99 += [N99/Nunits]\n",
    "            all10 = set.intersection(*map(set,top10))\n",
    "            all50 = set.intersection(*map(set,top50))\n",
    "            pct10 = len(all10)*100/Nunits\n",
    "            min10 = min(min10,pct10)\n",
    "            max10 = max(max10,pct10)\n",
    "            pct50 = len(all50)*100/Nunits\n",
    "            min50 = min(min50,pct50)\n",
    "            max50 = max(max50,pct50)\n",
    "            if 'greedy' in method:\n",
    "                min99 = min(min99,mean(pct99))\n",
    "                max99 = max(max99,mean(pct99))\n",
    "\n",
    "    print('%0.3f %0.3f'%(min10,max10))\n",
    "    print('%0.3f %0.3f'%(min50,max50))\n",
    "    if 'greedy' in method:\n",
    "        print('%0.3f %0.3f'%(min99,max99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does decoding performance degrade? \n",
    "\n",
    " - Get top 10%, 30%, 50%, 100%\n",
    " - Compute Growth of NMSE in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@memoize\n",
    "def get_all_xy_data(animal,sessions,uu):\n",
    "    xdata,ydata = [],[]\n",
    "    for session in sessions:\n",
    "        t,y    = get_neural_signals_for_training(animal,session)\n",
    "        us     = good_units_index(animal,session)\n",
    "        assert(set(uu)&set(us)==set(uu))\n",
    "        xdata += [get_in_trial(get_y(animal,session),animal,session,dozscore=True)]\n",
    "        ydata += [y[:,[where(us==u)[0][0] for u in uu]]]\n",
    "    return xdata,ydata\n",
    "\n",
    "def top_K_percent(animal,session,method,reg,PCT):\n",
    "    err0, emax, errs, Δerrs, uids, order = results[animal,session,method,reg]\n",
    "    Nunits = len(order)\n",
    "    Npct   = int(Nunits*PCT/100)\n",
    "    top    = order[:Npct]\n",
    "    return top\n",
    "\n",
    "def get_inter_day_generalization_errors(animal,session,sessions,method,reg,PCT):\n",
    "    # Identify top percent of units\n",
    "    top = top_K_percent(animal,session,method,reg,PCT)  \n",
    "    # Collect data for these neuronss\n",
    "    xdata, ydata = get_all_xy_data(animal,sessions,top)\n",
    "    # Fit model on target session, first half\n",
    "    i = where(session==array(sessions))[0][0]\n",
    "    T = xdata[i].shape[0]\n",
    "    w = reglstsq(ydata[i][:T//2,:],xdata[i][:T//2],reg=reg).ravel()\n",
    "    # Only test on second half of target sessions\n",
    "    ydata[i] = ydata[i][T//2:,:]\n",
    "    xdata[i] = xdata[i][T//2:]\n",
    "    # Test model on all days\n",
    "    ee = array([mean((x-y@w)**2) for x,y in zip(xdata,ydata)])\n",
    "    # Get actual elapsed days between sessions\n",
    "    daymap = dict(zip(sessions,get_days(animal)))\n",
    "    days   = [daymap[s] for s in sessions]\n",
    "    Δdmax  = days[-1]-days[0]\n",
    "    # Average ?\n",
    "    Δderrs = defaultdict(list)\n",
    "    for j in range(len(sessions)):\n",
    "        Δdays = abs(days[i]-days[j])\n",
    "        Δderrs[Δdays] += [ee[j]]\n",
    "    Δdays = array(sorted(list(Δderrs.keys())))\n",
    "    Δerrs = array([mean(Δderrs[i]) for i in Δdays])\n",
    "    return Δdays,Δerrs,Δderrs\n",
    "\n",
    "@memoize\n",
    "def get_day_error_growth(animal,sessions,method,PCT,reg):\n",
    "    units  = get_units_in_common(animal,sessions)[0]\n",
    "    # Each session has its own set of top units _\n",
    "    Nsessions = len(sessions)\n",
    "    all_Δderrs = defaultdict(list)\n",
    "    for i,session in enumerate(sessions):\n",
    "        Δdays,Δerrs,Δderrs = get_inter_day_generalization_errors(animal,session,sessions,method,reg,PCT)\n",
    "        for Δ,ee in Δderrs.items():\n",
    "            all_Δderrs[Δ] += ee\n",
    "    Δdays = array(sorted(list(all_Δderrs.keys())))\n",
    "    Δerrs = array([mean(all_Δderrs[i]) for i in Δdays])\n",
    "    print('hi')\n",
    "    return Δdays,Δerrs,all_Δderrs\n",
    "\n",
    "method = 'descending_greedy'\n",
    "#method = 'single_cell'\n",
    "#method = 'ascending_greedy'\n",
    "#method = 'leave_one_out'\n",
    "reg    = 1e-5\n",
    "animal, sessions = use[2]\n",
    "Nsessions = len(sessions)\n",
    "\n",
    "subplot(221)\n",
    "for i,PCT in enumerate((10,30,50,100)):\n",
    "    Δdays,Δerrs,all_Δderrs = get_day_error_growth(animal,sessions,method,PCT,reg)\n",
    "    plot(Δdays,Δerrs,color=riley(i*0.8/4),label='%d%%'%PCT)\n",
    "\n",
    "simpleraxis()\n",
    "xlabel('Δ days')\n",
    "ylabel('NMSE')\n",
    "rightlegend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify good SNR signals as in Driscoll et al. \n",
    "\n",
    " - You'll need to get position-triggered averages of the signal\n",
    " - You'll need to identify cells that are peaked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SNR_metrics(animal,session,units,\n",
    "                    N=100,\n",
    "                    threshold=0.1,\n",
    "                    doplot=False,\n",
    "                    miny = -1.8,\n",
    "                    maxy = 1.8):\n",
    "    times,ydata = get_neural_signals_for_training(animal,session)\n",
    "    xdata       = get_in_trial(get_y(animal,session),animal,session,dozscore=True)\n",
    "    avail_units = good_units_index(animal,session)\n",
    "    units       = array(sorted(list(set(units))))\n",
    "    pick        = array([where(avail_units==u)[0][0] for u in units])\n",
    "    ydata       = ydata[:,pick]\n",
    "    xdata       = cat(extract_in_trial(xdata,animal,session,dozscore=True))\n",
    "    ydata       = cat(extract_in_trial(ydata,animal,session,dozscore=True))\n",
    "    edges   = linspace(miny,maxy,N+1)\n",
    "    centers = (edges[:-1]+edges[1:])/2\n",
    "    μ,σ,m = [],[],[]\n",
    "    for a,b in zip(edges[:-1],edges[1:]):\n",
    "        i = (xdata>=a)&(xdata<b)\n",
    "        y = ydata[i,:]\n",
    "        μ.append(np.sum(y,axis=0))\n",
    "        σ.append(np.sum(y**2,axis=0))\n",
    "        m.append(len(y))\n",
    "    μ = array(μ)\n",
    "    σ = array(σ)\n",
    "    m = array(m)+0.5\n",
    "    smooth = exp(-linspace(-8,8,N)**2)\n",
    "    smooth/= sum(smooth)\n",
    "    smooth = toeplitz(fftshift(smooth))\n",
    "    smooth/= sum(smooth,1)[:,None]\n",
    "    μ = smooth @ μ\n",
    "    σ = smooth @ σ\n",
    "    m = smooth @ m\n",
    "    μ = μ/m[:,None]\n",
    "    σ = sqrt( σ/m[:,None] - μ**2)\n",
    "    ε = σ/sqrt(m)[:,None]*1.96\n",
    "\n",
    "    peaks = argmax(μ,axis=0)\n",
    "    SNRs  = array([mean(μi-εi>threshold) for μi,εi in zip(μ.T,ε.T)])\n",
    "    tuned = find(SNRs>threshold)\n",
    "    modix = mean((μ/σ)**2,axis=0)**0.5\n",
    "    \n",
    "    if doplot:\n",
    "        figure(figsize=(TEXTWIDTH,)*2)\n",
    "        #tuned = arange(len(units)) ## REMOVE THIS LATER\n",
    "        tuned  = argsort(modix)\n",
    "        ntuned = len(tuned)\n",
    "        k = int(ceil(sqrt(ntuned)))\n",
    "        for j,i in enumerate(tuned):\n",
    "            subplot(k,k,j+1)\n",
    "            title(i)\n",
    "            u = array(μ)[:,i]\n",
    "            plot(centers,u,lw=0.6);\n",
    "            e = array(ε)[:,i]\n",
    "            fill_between(centers,u-e,u+e,lw=0.6,alpha=0.25,color=TURQUOISE);\n",
    "            axhline(0,lw=1,linestyle=':')\n",
    "            noxyaxes();\n",
    "            simpleraxis()\n",
    "        tight_layout()\n",
    "        \n",
    "    return peaks, SNRs, tuned, modix\n",
    "\n",
    "animal,sessions = use[0]\n",
    "units,uidxs = get_units_in_common(animal,sessions)\n",
    "session     = sessions[0]\n",
    "peaks, SNRs, tuned, modix = get_SNR_metrics(animal,session,units,doplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how ranking stability relates to SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'single_cell'\n",
    "method = 'descending_greedy'\n",
    "reg    = 1e-3\n",
    "\n",
    "animal,sessions = use[1]\n",
    "#for animal,sessions in use:    \n",
    "units,uidxs = get_units_in_common(animal,sessions)\n",
    "Nunits = len(units)\n",
    "all_peak, all_SNRs, all_tuned, all_modix = list(map(array,zip(*[get_SNR_metrics(animal,s,units) for s in sessions])))\n",
    "best = find(np.min(all_SNRs,axis=0)>0.05)\n",
    "all_rank = [] \n",
    "for session in sessions:\n",
    "    err0, emax, errs, Δerrs, order_idx, order_units = results[animal,session,method,reg]\n",
    "    order_idx = array(order_idx)\n",
    "    rank      = array([find(order_idx==i) for i in range(Nunits)]).squeeze()\n",
    "    all_rank.append(rank)\n",
    "all_rank = array(all_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These have consistently sharp peaks\n",
    "minSNR = np.min(all_SNRs,axis=0)\n",
    "print('Median minimum SNR is',median(minSNR))\n",
    "best = find(minSNR>0.025)\n",
    "avg  = find(np.mean(all_SNRs,axis=0)>0.1)\n",
    "\n",
    "# These have qualitatively stable peak locatoins\n",
    "peak_Δ = np.max(all_peak,axis=0)-np.min(all_peak,axis=0)\n",
    "stable = find(peak_Δ<30)\n",
    "\n",
    "# Report some numbers\n",
    "a = set(avg)\n",
    "b = set(best)\n",
    "s = set(stable)\n",
    "print('')\n",
    "print('%d cells (%d%%) are strongly tuned on average'%(len(a),100*len(a)/len(units)))\n",
    "print('%d cells (%d%%) are consistently strongly tuned'%(len(b),100*len(b)/len(units)))\n",
    "print('%d cells (%d%%) are have stable peaks'%(len(s),100*len(s)/len(units)))\n",
    "print('%2.1f%% of consistently strongly-tuned cells have stable peaks'%(100*len(b&s)/len(b)))\n",
    "print('%2.1f%% of cells with stable peaks are consistently strongly tuned'%(100*len(b&s)/len(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These have at least one sharp peak\n",
    "maxSNR = np.max(all_SNRs,axis=0)\n",
    "print('Median maximum SNR is',median(maxSNR))\n",
    "best = find(maxSNR>0.25)\n",
    "avg  = find(np.mean(all_SNRs,axis=0)>0.1)\n",
    "\n",
    "# These have qualitatively stable peak locatoins\n",
    "peak_Δ = np.max(all_peak,axis=0)-np.min(all_peak,axis=0)\n",
    "stable = find(peak_Δ<30)\n",
    "\n",
    "# Report some numbers\n",
    "a = set(avg)\n",
    "b = set(best)\n",
    "s = set(stable)\n",
    "print('')\n",
    "print('%d cells (%d%%) are strongly tuned on average'%(len(a),100*len(a)/len(units)))\n",
    "print('%d cells (%d%%) are strongly tuned sometimes'%(len(b),100*len(b)/len(units)))\n",
    "print('%d cells (%d%%) are have stable peaks'%(len(s),100*len(s)/len(units)))\n",
    "print('%2.1f%% of sometimes strongly-tuned cells have stable peaks'%(100*len(b&s)/len(b)))\n",
    "print('%2.1f%% of cells with stable peaks are sometimes strongly tuned'%(100*len(b&s)/len(s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sorting based only on top 30% most modulated cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOMPUTE_RESULTS = False\n",
    "\n",
    "saveto = \"./datafiles/importance_sorting_result_top_modulated_only.p\"\n",
    "\n",
    "if RECOMPUTE_RESULTS:\n",
    "    results_modulated = {}\n",
    "    for animal,sessions in use:\n",
    "        print('Subject',animal)\n",
    "        units,uidxs = get_units_in_common(animal,sessions)\n",
    "\n",
    "        Nunits = len(units)\n",
    "        all_peak, all_SNRs, all_tuned, all_modix = list(map(array,zip(*[get_SNR_metrics(animal,s,units) for s in sessions])))\n",
    "        # Let's select cells based on modulation index\n",
    "        maxMIX  = np.max(all_modix,axis=0)\n",
    "        mmm     = percentile(maxMIX,30)\n",
    "        study   = maxMIX>mmm\n",
    "        units   = array(units)[study]\n",
    "        \n",
    "        for session in progress_bar(sessions):\n",
    "            for method in methods:\n",
    "                for reg in regstrengths: \n",
    "                    results_modulated[animal,session,method,reg] = (units,get_importance_forward_position(animal,session,units,reg,method,NXVAL))\n",
    "                    # err0, emax, errs, Δerrs, uids, units[uids]\n",
    "            pickle.dump(results_modulated,open(saveto,'wb'))\n",
    "else:\n",
    "    results_modulated = pickle.load(open(saveto,'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check: are ranking results any different now? \n",
    "\n",
    "Not really"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 1e-3\n",
    "print('Regularization strength %0.0e:'%reg)\n",
    "for method in methods:\n",
    "    print('\\nUsing method:',method)\n",
    "    print('==============================')\n",
    "\n",
    "    for animal,sessions in use:\n",
    "        print('\\nM%d (%2d sessions spanning %2d days)'%(animal,len(sessions),sessions[-1]-sessions[0]))\n",
    "        print('---------------------------------')\n",
    "\n",
    "        top10 = []\n",
    "        top50 = []\n",
    "        errs  = []\n",
    "        err30 = []\n",
    "        pct99 = []\n",
    "        for session in sessions:\n",
    "            err0, emax, errs, Δerrs, uids, units = results_modulated[animal,session,method,reg][1]\n",
    "            Nunits = len(units)\n",
    "            N10    = int(Nunits/10)\n",
    "            N30    = int(Nunits/3)\n",
    "            N50    = int(Nunits/2)\n",
    "            top10 += [units[:N10]]\n",
    "            top50 += [units[:N50]]\n",
    "            err30 += [errs[N30]/emax]\n",
    "            errs  += [err0/emax]\n",
    "            if 'greedy' in method:\n",
    "                N99    = find((1-errs/emax)>=(1-err0/emax)*0.99)[0]\n",
    "                pct99 += [N99/Nunits]\n",
    "        all10 = set.intersection(*map(set,top10))\n",
    "        all50 = set.intersection(*map(set,top50))\n",
    "        print(' - Cells always in the top 10%% : %2d units, %2d%%'%(len(all10),len(all10)*100/Nunits))\n",
    "        print(' - Cells always in the top 50%% : %2d units, %2d%%'%(len(all50),len(all50)*100/Nunits))\n",
    "        if 'greedy' in method:\n",
    "            print(' - Average NMSE using all cells: %2d%% ±%2dσ'%(mean(errs)*100,std(errs)*100))\n",
    "            print(' - Average NMSE using top 30%%  : %2d%%'%(100*mean(err30)))\n",
    "            print(' - 30%% of cells account for %d%% of variance explained by full population'%(100*(1-mean(err30))/(1-mean(errs))))\n",
    "            print(' - %2d%% of cells account for 99%% of variance explained by full population'%(100*mean(pct99)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures we can correlate\n",
    "\n",
    "Check how single-cell, pull-one-out, and modulation index compare. \n",
    "Can any of these be used to predict the other in the future? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 1e-3\n",
    "animal,sessions = use[1]\n",
    "units,uidxs = get_units_in_common(animal,sessions)\n",
    "\n",
    "Nunits = len(units)\n",
    "all_peak, all_SNRs, all_tuned, all_modix = list(map(array,zip(*[get_SNR_metrics(animal,s,units) for s in sessions])))\n",
    "        \n",
    "Δerrs1 = array([results[animal,session,'single_cell'  ,reg][3] for session in sessions])\n",
    "uids1  = array([results[animal,session,'single_cell'  ,reg][4] for session in sessions])\n",
    "Δerrs2 = array([results[animal,session,'leave_one_out',reg][3] for session in sessions])\n",
    "uids2  = array([results[animal,session,'leave_one_out',reg][4] for session in sessions])\n",
    "\n",
    "# Convert ranked orderings to list of ranks\n",
    "pos1   = array([[find(uu==i)[0] for i in range(Nunits)] for uu in uids1])\n",
    "pos2   = array([[find(uu==i)[0] for i in range(Nunits)] for uu in uids2])\n",
    "# Re-order the Δε\n",
    "Δε1 = array([[e[p]] for e,p in zip(Δerrs1,pos1)]).squeeze()\n",
    "Δε2 = array([[e[p]] for e,p in zip(Δerrs2,pos2)]).squeeze()\n",
    "\n",
    "subplot(411)\n",
    "imshow(Δε1,aspect='auto')\n",
    "noxyaxes()\n",
    "title('Single Cell Δerror')\n",
    "subplot(412)\n",
    "imshow(Δε2,aspect='auto')\n",
    "noxyaxes()\n",
    "title('Pull-one-out Δerror')\n",
    "subplot(413)\n",
    "imshow(all_modix,aspect='auto')\n",
    "title('Modulation Index')\n",
    "noxyaxes()\n",
    "subplot(414)\n",
    "imshow(-abs(zeromean(all_peak,axis=0)),aspect='auto')\n",
    "title('Peak Shift Range Magnitude')\n",
    "noxyaxes()\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot(221)\n",
    "scatter(np.min(Δε1,0),np.min(Δε2,0))\n",
    "simpleaxis(); xlabel('Δε1'); ylabel('Δε2'); \n",
    "subplot(222)\n",
    "scatter(np.min(all_modix,0),np.min(Δε2,0))\n",
    "simpleaxis(); xlabel('modulation'); ylabel('Δε2'); \n",
    "subplot(223)\n",
    "scatter(np.min(Δε1,0),np.min(all_modix,0))\n",
    "simpleaxis(); xlabel('Δε1'); ylabel('modulation'); \n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how much different critera overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Δpeak = np.max(all_peak,axis=0)-np.min(all_peak,axis=0)\n",
    "stable    = find(Δpeak<20)\n",
    "modulated = find(np.min(all_modix,0)>0.1)\n",
    "minΔε     = np.min(Δε1,0)\n",
    "important = find(minΔε>median(minΔε))\n",
    "s = set(stable   )\n",
    "m = set(modulated)\n",
    "i = set(important)\n",
    "ns,nm,ni = len(s),len(m),len(i)\n",
    "print(ns,nm,ni)\n",
    "print('Fraction stable    cells that are modulated',len(s&m)/ns)\n",
    "print('Fraction modulated cells that are stable   ',len(s&m)/nm)\n",
    "print('Fraction stable    cells that are important',len(s&i)/ns)\n",
    "print('Fraction important cells that are stable   ',len(s&i)/ni)\n",
    "print('Fraction modulated cells that are important',len(m&i)/nm)\n",
    "print('Fraction important cells that are modulated',len(m&i)/ni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking by modulation index is also unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Modulation index rank stability')\n",
    "print('M%d sessions'%animal,sessions)\n",
    "m      = all_modix\n",
    "mxrank = argsort(-m,axis=1)\n",
    "mxrank[:,:Nunits//10]\n",
    "print('%2d%% always in top 10'%\\\n",
    "      (100*len(set.intersection(*map(set,mxrank[:,:Nunits//10])))/Nunits))\n",
    "print('%2d%% always in top 50'%\\\n",
    "      (100*len(set.intersection(*map(set,mxrank[:,:Nunits//2 ])))/Nunits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Plot the variance of a ranking as a function of its mean; Do good cells tend to stay good-ish? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get various statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'single_cell'\n",
    "#method = 'ascending_greedy'\n",
    "reg    = 1e-3\n",
    "\n",
    "animal,sessions = use[1]\n",
    "#for animal,sessions in use:    \n",
    "units,uidxs = get_units_in_common(animal,sessions)\n",
    "Nunits = len(units)\n",
    "all_peak, all_SNRs, all_tuned, all_modix = list(map(array,zip(*[get_SNR_metrics(animal,s,units) for s in sessions])))\n",
    "all_rank = [] \n",
    "for session in sessions:\n",
    "    err0, emax, errs, Δerrs, order_idx, order_units = results[animal,session,method,reg]\n",
    "    all_rank.append(array([find(array(order_idx)==i) for i in range(Nunits)]).squeeze())\n",
    "all_rank = array(all_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check how variance of metrics changes for good/bad cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subplot(221)\n",
    "x = np.mean(all_rank/Nunits,0)\n",
    "y = var(all_rank/Nunits,0)\n",
    "scatter(x,y,s=5,clip_on=False)\n",
    "simpleaxis()\n",
    "title('Ascending greedy ranking stability')\n",
    "xlabel('Mean rank\\n(lower = more important)')\n",
    "ylabel('Rank variance')\n",
    "xlim(0,1); ylim(0,0.125)\n",
    "x = linspace(0,1,100)\n",
    "plot(x,x*(1-x)/4,color=TURQUOISE,zorder=-inf)\n",
    "\n",
    "subplot(222)\n",
    "x = np.mean(all_modix,0)\n",
    "y = std(all_modix,0)#/mean(all_modix,0)\n",
    "scatter(x,y,s=5,clip_on=False)\n",
    "simpleaxis()\n",
    "title('Modulation index stability')\n",
    "xlabel('Mean over days\\n(higher = stronger task modulation)')\n",
    "#ylabel('Coefficient of\\nvariation σ/μ')\n",
    "ylabel('Standard deviation')\n",
    "\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = mean(all_modix,0)\n",
    "o  = argsort(mm)\n",
    "mx = all_modix[:,o]\n",
    "mm = mm[o]\n",
    "ee = std(mx,0)*1.96\n",
    "m1 = np.min(all_modix,0)[o]\n",
    "m2 = np.max(all_modix,0)[o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(arange(Nunits),mm,s=5)\n",
    "for i,(m1i,m2i) in enumerate(zip(m1,m2)):\n",
    "    plot([i,i],[m1i,m2i],lw=0.8,color=BLACK)\n",
    "simpleraxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Thoughts? \n",
    "\n",
    " - Variability is high\n",
    " - Middle 50% of cells extremely variable and unstable\n",
    " - Variance in modulation does not decrease or high-quality cells\n",
    " \n",
    "Overall this is consistent with rankings being highly unstable. Nevertheless Driscoll et al. found that 40% of task-modulated cells had stable peaks. This does not mean that the usefulness of these cells for decoding was also stable. We need to better dissociate various effects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the so-called stable cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_peak.shape\n",
    "\n",
    "stability = std(all_peak,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(mean(all_modix,0),stability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal,sessions = use[1]\n",
    "# Load data\n",
    "times,ydata = get_neural_signals_for_training(animal,session)\n",
    "avail_units = good_units_index(animal,session)\n",
    "xdata       = get_in_trial(get_y(animal,session),animal,session,dozscore=True)\n",
    "\n",
    "units = array(sorted(list(set(units))))\n",
    "assert(set(units)==(set(avail_units)&set(units)))\n",
    "pick  = array([where(avail_units==u)[0][0] for u in units])\n",
    "ydata = ydata[:,pick]\n",
    "u2    = avail_units[pick]\n",
    "assert(all(u2==units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If neuron becomes relatively quieter or less reliable then the weight assigned may become inappropriate for decoding. this affects our analyses, and would also physiologically affect a downstream neuron with fixed synaptic weights.\n",
    "\n",
    " - Show that stablly-tuned neurons have unstable SNR \n",
    " - or unstable rates\n",
    " - These are equivalent for normalized signals, hmm... so I guess SNR is the only proxy\n",
    " \n",
    "We define the Signal to Noise Ratio (SNR) for a single-cell tuning curve as the root mean-squared ratio of the location-triggered mean rate to location-triggered variance. \n",
    "\n",
    "$$\n",
    "k^2 = \\left<\\frac{\\mu^2}{\\sigma^2}\\right>\n",
    "$$\n",
    "\n",
    "Data checks out:\n",
    "\n",
    "For example, no more than 8% of neurons that were in the top 20% in terms of tuning-curve stability were also consistently in the top 25% in terms of SNR. \n",
    "\n",
    "If neuron becomes relatively quieter or less reliable then the weight assigned may become inappropriate for decoding. this affects our analyses, and would also physiologically affect a downstream neuron with fixed synaptic weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method = 'single_cell'\n",
    "method = 'ascending_greedy'\n",
    "reg    = 1e-3\n",
    "\n",
    "pct    = 25\n",
    "\n",
    "animal,sessions = use[1]\n",
    "for animal,sessions in use:    \n",
    "    print('Mouse',animal)\n",
    "    units,uidxs = get_units_in_common(animal,sessions)\n",
    "    Nunits = len(units)\n",
    "    all_peak, _, all_tuned, all_SNRs = list(map(array,zip(*[get_SNR_metrics(animal,s,units) for s in sessions])))\n",
    "    Δpeaks = np.max(all_peak,0)-np.min(all_peak,0)\n",
    "    stable_peaks = Δpeaks < percentile(Δpeaks,pct)\n",
    "    print('%d%%'%(100*mean(np.min(all_SNRs[:,stable_peaks],axis=0)>percentile(all_SNRs.ravel(),100-pct))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, no more than 8% of neurons that were in the top 20% in terms of tuning-curve stability were also consistently in the top 25% in terms of SNR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between 8-36% of neurons that had above-median stability also consistently had above-median SNR. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
